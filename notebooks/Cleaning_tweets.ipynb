{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A- Clean kaggle data (only US and France):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_tweets = pd.read_csv('../data/tweets_us/kaggle_en_tweets/vaccination_all_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69718, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340539111971516416</td>\n",
       "      <td>Rachel Roh</td>\n",
       "      <td>La Crescenta-Montrose, CA</td>\n",
       "      <td>Aggregator of Asian American news; scanning diverse sources 24/7/365. RT's, Follows and 'Likes' will fuel me üë©‚Äçüíª</td>\n",
       "      <td>2009-04-08 17:52:46</td>\n",
       "      <td>405</td>\n",
       "      <td>1692</td>\n",
       "      <td>3247</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20 06:06:44</td>\n",
       "      <td>Same folks said daikon paste could treat a cytokine storm #PfizerBioNTech https://t.co/xeHhIMg1kF</td>\n",
       "      <td>['PfizerBioNTech']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1338158543359250433</td>\n",
       "      <td>Albert Fong</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Marketing dude, tech geek, heavy metal &amp; '80s music junkie. Fascinated by meteorology and all things in the cloud. Opinions are my own.</td>\n",
       "      <td>2009-09-21 15:27:30</td>\n",
       "      <td>834</td>\n",
       "      <td>666</td>\n",
       "      <td>178</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-13 16:27:13</td>\n",
       "      <td>While the world has been on the wrong side of history this year, hopefully, the biggest vaccination effort we've ev‚Ä¶ https://t.co/dlCHrZjkhm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id    user_name              user_location  \\\n",
       "0  1340539111971516416   Rachel Roh  La Crescenta-Montrose, CA   \n",
       "1  1338158543359250433  Albert Fong          San Francisco, CA   \n",
       "\n",
       "                                                                                                                          user_description  \\\n",
       "0                         Aggregator of Asian American news; scanning diverse sources 24/7/365. RT's, Follows and 'Likes' will fuel me üë©‚Äçüíª   \n",
       "1  Marketing dude, tech geek, heavy metal & '80s music junkie. Fascinated by meteorology and all things in the cloud. Opinions are my own.   \n",
       "\n",
       "          user_created  user_followers  user_friends  user_favourites  \\\n",
       "0  2009-04-08 17:52:46             405          1692             3247   \n",
       "1  2009-09-21 15:27:30             834           666              178   \n",
       "\n",
       "   user_verified                 date  \\\n",
       "0          False  2020-12-20 06:06:44   \n",
       "1          False  2020-12-13 16:27:13   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                                             Same folks said daikon paste could treat a cytokine storm #PfizerBioNTech https://t.co/xeHhIMg1kF   \n",
       "1  While the world has been on the wrong side of history this year, hopefully, the biggest vaccination effort we've ev‚Ä¶ https://t.co/dlCHrZjkhm   \n",
       "\n",
       "             hashtags               source  retweets  favorites  is_retweet  \n",
       "0  ['PfizerBioNTech']  Twitter for Android         0          0       False  \n",
       "1                 NaN      Twitter Web App         1          1       False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_tweets = kaggle_tweets.dropna(subset = ['user_location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Rename and drop columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_tweets = (kaggle_tweets\n",
    "          .drop(columns = ['user_created', 'user_favourites', 'source', 'favorites', 'is_retweet'])\n",
    "          .rename(columns = {'id': 'tweet_id'})\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Keep only tweets in France and USA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of user location (higher count on top)  \n",
    "#list_cities = kaggle_tweets.groupby('user_location')['user_location'].count().sort_values(ascending=False).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lists of places of interest for the study (France, south and north of USA)\n",
    "cities_fr = ['France', 'Paris, France', 'Nancy, France', 'Dammam, KSA | Tunisia | France', 'Alsace, France', 'Paris', 'La Belle Province', 'Bordeaux, France', 'Alpes-Maritimes, Provence-Alpe', '#CoronavirusMadeInPasteur', 'Rome, Italy - Beaulieu, France', 'Europe (usually Poland/France)', 'Paris, Limoges. France']\n",
    "cities_us_north = ['La Crescenta-Montrose, CA', 'San Francisco, CA', 'Los Angeles, CA', 'Rochester, NY', 'Cranbury, NJ', 'USAüá∫üá∏', 'America', 'Medford, OR', 'LI, NYC, NJ, PA, NE, USA', 'Chicago, IL', 'Seattle, WA',  'New Jersey, USA', 'Oregon, USA', 'California, USA', 'New York, NY', 'Cambridge, MA', 'New Orleans, LA', 'Michigan', 'Philadelphia, Pa.', 'North America', 'Troy, Michigan;  Serial ‚úàÔ∏è', 'Pennsylvania, USA', 'Saugerties, New York', 'Balboa Peninsula, SoCal USA', 'New York / New Jersey / Global', 'Minneapolis, MN', 'Vermont, US', 'Naperville, IL', 'Sacramento', 'New York, USA', 'Cherry Hill, NJ', 'NYC', 'San Jose, CA (USA)', 'Colorado Springs, Colorado ', 'Silicon Valley, CA', 'Washington, DC', 'D.C By Way Of BOSTON!üá≥üá¨', 'New York', 'Mequon, Wisconsin', 'Boston, MA', 'New York City, Westport, CT', 'Michigan, USA', 'US - IL - ID', 'Washington DC', 'CHICAGO!', 'NY', 'Brooklyn, NY', 'Los Angeles', 'Providence, RI', 'Boston', 'New York City', 'Rancho Cucamonga, CA', 'Syracuse, NY', 'New York & San Francisco', 'Philadelphia, PA', 'New Jersey', 'Buffalo, NY', 'Connecticut, USA', 'Rhode Island, USA', 'Northern Illinois', 'Rochester NY', 'Southern CA', 'Staten Island, New York', 'South Lake Tahoe, CA', 'Boston, MA.', 'washington', 'Paramus, NJ', 'Orange County, CA', 'Elmhurst, IL', 'Grand Rapids, MI', 'Long Beach, CA', 'Washington, D.C.', 'Northern California, USA', 'Princeton, NJ', 'Ventura, CA ', 'Washington D.C.', 'Rochester, MN', 'Denver, CO', 'Englewood, NJ', 'Katonah, NY', 'Chicago', 'Valencia,CA', 'Bellevue, WA', 'Minnesota, USA', 'Old Mystic, CT USA', 'San Francisco Bay Area', 'Santa Barbara, CA', 'Baltimore', 'Vermont, USA', 'Sunnyvale, CA', 'Bethesda, MD.', 'Queens, NY', 'Rochester MN', 'Social distancing in Manhattan', 'Morristown, NJ', 'Bronx, NY', 'DC Metro Area, USA', 'Maryland, USA', 'Stony Brook,  NY', 'Bryn Mawr, PA', 'Madison, WI', 'Coventry Rhode Island', 'Ann Arbor, ‚úã', 'Colorado, USA', 'Baltimore, MD', 'Pittsburgh, PA', 'Wisconsin, USA', 'minneapolis', 'Southern California', 'Northern California', 'Potomac  MD', 'Seaside, California, USA', 'Hartford, CT', 'Long Island   ', 'California', 'Paterson, NJ', 'NH', 'Manhattan, NYC', 'California Bay Area', 'VT', 'Stanford, CA', 'Duarte, CA', 'Los Angeles, California', 'Central California', 'Portland, ME', 'Philadelphia', 'Riverdale Park, MD ', 'Manhattan, NY', 'East Hampton, NY', 'Hollywood, CA, USA', 'South Cal', 'Ann Arbor, MI', 'Batavia, IL', 'Cambridge, MA, United States', 'Chicago-adjacent, IL', 'Metro North of Boston MA', 'New Haven, CT', 'Silver Spring, MD', 'West Hollywood, CA', 'So Cal', 'SoCal', 'California, United States', 'Sacramento, CA', 'Massachusetts, USA', 'Illinois, USA', 'Washington, USA', 'Fremont, CA, USA', 'New Jersey', 'Portland, OR', 'San Francisco', 'New York, New York', 'Santa Monica, CA', 'Connecticut', 'West Hartford, CT', 'Oakland, CA', 'Anaheim, CA', 'San Jose, CA', 'Buffalo, NY', 'Massachusetts', 'Los Angeles, California', 'Vermont, USA', 'Detroit, MI', 'Delaware, USA', 'Philadelphia', 'Boothbay Harbor, Maine', 'San Diego, California']\n",
    "cities_us_south = ['Texas', 'Jefferson City, MO',  'LA | OC ', 'Beaumont, Texas', 'Kentucky', 'Dallas, TX', 'Lakeland, FL', 'Palm Beach, FL', 'Tampa, FL', 'Okolona, KY', 'Houston, TX', 'Miami, FL', 'McAllen, TX', 'Texas, USA', 'Miami', 'Charleston, SC', 'Florida, USA', 'West Virginia, USA', 'Raleigh, NC', 'Raleigh, NC, USA', 'South Carolina, USA', 'Durham, NC', 'Alabama', 'Cincinnati, OH ', '11150A Asheville Hwy Inman, SC', 'Tucson, AZ', 'Albuquerque, NM', 'Nashville, TN', 'St Petersburg, FL', 'Austin, TX', 'Kansas', 'Roanoke, VA', 'Clinton, SC', 'Miami Beach, FL', 'San Antonio, Texas', 'Atlanta, GA', 'Odessa, TX', 'Chapel Hill, NC', 'Scottsdale, AZ', 'NEVADA', 'Richmond, VA', 'Atlanta, Georgia', 'Tysons Corner, Virginia', 'Phoenix, AZ', 'Greensboro, NC', 'Las Vegas, NV', 'Harlingen, TX', 'North Carolina, USA', 'Indianapolis, IN', 'Spring, TX', 'Idaho, USA', 'Athens, GA', 'Jacksonville, FL', 'Houston Texas Based', 'Jackson, MS', 'Covid Hell, AL', 'Kansas City, KS', 'Fort Worth', 'Charlottesville, VA', 'Kansas City, MO', 'Florida', 'Dayton, OH', 'Saint Louis, MO, USA', 'St Louis, MO', 'Knoxville, TN', 'Birmingham, AL', 'Columbia, MO', 'Morgantown, WV', 'TX', 'Allen, TX', 'Arkansas', 'Gainesville, FL', 'Smithville, TX', 'Augusta, GA', 'West Texas', 'Oklahoma City & Tulsa, OK', 'Oklahoma, USA', 'Gainesville, FL, USA', 'Arkansas, United States', 'Sugar Land, TX', 'Miami, Fl', 'Central FL', 'arkansas', 'Riverdale, UT', 'Nashville', 'Dallas', 'Houston', 'Reno, NV', 'Tampa Bay, FL', 'Charlotte, NC', 'Austin', 'Kansas City ', 'South Florida', 'Salt Lake City, UT', 'Alabama, USA', 'Ohio, USA', 'Virginia, USA', 'Orlando, FL', 'Las Vegas', 'Lufkin, TX', 'Arizona, USA', 'Cleveland, OH', 'Georgia, USA', 'Fort Lauderdale, FL', 'New York, Sarasota, FL', 'Columbus, OH', 'Louisville, KY', 'Tennessee, USA', 'Missouri, USA', 'Indiana, USA', 'Kentucky, USA', 'Oklahoma City, OK', 'North Carolina', 'New Mexico, USA', 'Rio Grande Valley, TX', 'Houston, Tx', 'Ohio', 'Cleveland, Ohio', 'Utah, USA', 'Indiana', 'Salt Lake City, Utah', 'Dallas, Texas', 'South Carolina', 'Columbus, Ohio', 'Corpus Christi, TX', 'Tampa, FL - United States', 'Atlanta', 'Indianapolis', 'Salt Lake City, UT', 'Alabama', 'TX', 'Fort Worth, TX', 'Jacksonville, N.C.', 'Tennessee', 'Arkansas, USA', 'Baton Rouge, LA', 'Arkansas', 'Jupiter, FL', 'Louisiana, USA', 'Tulsa, OK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dfs with the list of cities\n",
    "tweets_fr = kaggle_tweets[kaggle_tweets['user_location'].isin(cities_fr)]\n",
    "tweets_us_north = kaggle_tweets[kaggle_tweets['user_location'].isin(cities_us_north)]\n",
    "tweets_us_south = kaggle_tweets[kaggle_tweets['user_location'].isin(cities_us_south)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B- Clean data from Tweepy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- France:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge files to one file and add a column with the respective vaccines names(week 2-3):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(667, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging astrazeneca files\n",
    "astra_fr = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_fr/astra/tweets_fr_tweets_fr_aztra_2021_05_18_to_05_12.csv', '../data/tweets_fr/astra/tweets_fr_tweets_fr_astra_2021_05_19_to_05_13.csv']), ignore_index=True)\n",
    "astra_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column with vaccine name\n",
    "astra_fr['vaccine'] = 'astrazeneca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging J&J files\n",
    "jandj_fr = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_fr/JandJ/tweets_fr_jandj_2021_05_19_to_05_14.csv', '../data/tweets_fr/JandJ/tweets_fr_jandj_2021_05_18_to_05_11.csv']), ignore_index=True)\n",
    "jandj_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column with vaccine name\n",
    "jandj_fr['vaccine'] = 'johnson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2577, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging pfizer files\n",
    "pfizer_fr = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_fr/pfizer/tweets_fr_pfizer_2021_05_14_to_05_11.csv', '../data/tweets_fr/pfizer/tweets_fr_pfizer_2021_05_18_to_05_15.csv']), ignore_index=True)\n",
    "pfizer_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column with vaccine name\n",
    "pfizer_fr['vaccine'] = 'pfizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moderna\n",
    "moderna_fr = pd.read_csv('../data/tweets_fr/moderna/tweets_fr_moderna_2021_05_18_to_05_11.csv')\n",
    "moderna_fr['vaccine'] = 'moderna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes together\n",
    "vaccines_fr = pd.concat([astra_fr, jandj_fr, pfizer_fr, moderna_fr], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4111, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaccines_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweets with a different search\n",
    "all_fr = pd.read_csv('../data/tweets_fr/tweets_fr_all_2021_05_19_to_05_11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5099, 9)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = [\"Tweet Id\", \"Tweet Date\", \"location\", \"Follower Count\", \"Account Verified\", \"Favorite Count\", \"Retweets\", \"Tweet Text\", \"hashtags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tweets ID that are in common with the 2 different searchs\n",
    "common = all_fr.merge(vaccines_fr, how='inner', on=[\"Tweet Id\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3206, 18)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1907, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tweets that are not in vaccine_fr\n",
    "all_fr_no_com = all_fr[(~all_fr[\"Tweet Id\"].isin(common[\"Tweet Id\"]))]\n",
    "all_fr_no_com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a column with the respective vaccine for each tweet\n",
    "all_fr_no_com.loc[all_fr_no_com[\"Tweet Text\"].str.contains(\"pfizer|Pfizer\"), \"vaccine\"] = \"pfizer\"\n",
    "all_fr_no_com.loc[all_fr_no_com[\"Tweet Text\"].str.contains(\"moderna|Moderna\"), \"vaccine\"] = \"moderna\"\n",
    "all_fr_no_com.loc[all_fr_no_com[\"Tweet Text\"].str.contains(\"astrazeneca|Aztrazeneca\"), \"vaccine\"] = \"astrazeneca\"\n",
    "all_fr_no_com.loc[all_fr_no_com[\"Tweet Text\"].str.contains(\"johnson|Johnson\"), \"vaccine\"] = \"johnson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in column \"vaccine\"\n",
    "all_fr_no_com = all_fr_no_com.dropna(subset=[9], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all_fr_no_com to vaccines_fr\n",
    "vaccines_fr = pd.concat([vaccines_fr, all_fr_no_com], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and drop columns \n",
    "vaccines_fr = (vaccines_fr\n",
    "          .drop(columns = ['Account Verified', 'Favorite Count', 'hashtags'])\n",
    "          .rename(columns = {'Tweet Id': 'id', 'Tweet Date': 'date', 'Follower Count': 'follower_count', 'Retweets': 'retweets', 'Tweet Text': 'text'})\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"date\" object type to datetime\n",
    "vaccines_fr['date'] = vaccines_fr['date'].astype('Datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode 'utf-8' characters with real characters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use eval to decode \"utf-8\" and remove \"\\n\"\n",
    "vaccines_fr[\"text\"] = vaccines_fr[\"text\"].map(lambda x: eval(x).decode(\"utf-8\").replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Twitter Handles (@user) and links:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_text, pattern):\n",
    "    reg = re.findall(pattern, input_text)\n",
    "    for x in reg:\n",
    "        input_text = re.sub(x, '', input_text)\n",
    "        \n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter handles\n",
    "vaccines_fr[\"text\"] = np.vectorize(remove_pattern)(vaccines_fr['text'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links\n",
    "vaccines_fr[\"text\"] = np.vectorize(remove_pattern)(vaccines_fr['text'], \"https://[\\w\\.\\\\/]*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save \"vaccine_fr\" as a csv file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccines_fr.to_csv('../data/vaccines_fr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge files to one file and add a column with the respective vaccines names(week 4):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfizer_fr_week4 = pd.read_csv('../data/tweets_fr/pfizer/tweets_fr_pfizer_2021_05_29_to_05_22.csv')\n",
    "pfizer_fr_week4['vaccine'] = 'pfizer'\n",
    "moderna_fr_week4 = pd.read_csv('../data/tweets_fr/moderna/tweets_fr_moderna_2021_05_29_to_05_22.csv')\n",
    "moderna_fr_week4['vaccine'] = 'moderna'\n",
    "astra_fr_week4 = pd.read_csv('../data/tweets_fr/astra/tweets_fr_astra_2021_05_29_to_05_22.csv')\n",
    "astra_fr_week4['vaccine'] = 'astrazeneca'\n",
    "janssen_fr_week4 = pd.read_csv('../data/tweets_fr/JandJ/tweets_fr_jandj_2021_05_29_to_05_22.csv')\n",
    "janssen_fr_week4['vaccine'] = 'johnson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all_fr_no_com to vaccines_fr\n",
    "vaccines_fr_week4 = pd.concat([pfizer_fr_week4, moderna_fr_week4, astra_fr_week4, janssen_fr_week4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and drop columns \n",
    "vaccines_fr_week4 = (vaccines_fr_week4\n",
    "          .drop(columns = ['username', 'acctdesc', 'Friends Count','Account Verified', 'Favorite Count', 'hashtags'])\n",
    "          .rename(columns = {'Tweet Id': 'id', 'Tweet Date': 'date', 'Follower Count': 'follower_count', 'Retweets': 'retweets', 'Tweet Text': 'text'})\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"date\" object type to datetime\n",
    "vaccines_fr_week4['date'] = vaccines_fr_week4['date'].astype('Datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use eval to decode \"utf-8\" and remove \"\\n\"\n",
    "vaccines_fr_week4[\"text\"] = vaccines_fr_week4[\"text\"].map(lambda x: eval(x).decode(\"utf-8\").replace('\\n', ' '))\n",
    "# remove twitter handles\n",
    "vaccines_fr_week4[\"text\"] = np.vectorize(remove_pattern)(vaccines_fr_week4['text'], \"@[\\w]*\")\n",
    "# remove links\n",
    "vaccines_fr_week4[\"text\"] = np.vectorize(remove_pattern)(vaccines_fr_week4['text'], \"https://[\\w\\.\\\\/]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save file\n",
    "vaccines_fr_week4.to_csv('../data/vaccines_fr_week4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- USA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NORTHERN STATES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two csv files for week 3 and 4 of May\n",
    "pfizer_north = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_us/us_north/pfizer/tweets_us_north_pfizer_2021_05_22_to_05_16.csv', '../data/tweets_us/us_north/pfizer/tweets_us_north_pfizer_2021_05_31_to_05_23.csv']), ignore_index=True)\n",
    "# Make a new column with vaccine name\n",
    "pfizer_north['vaccine'] = 'pfizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two csv files for week 3 and 4 of May\n",
    "moderna_north = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_us/us_north/moderna/tweets_us_north_moderna_2021_05_22_to_05_16.csv', '../data/tweets_us/us_north/moderna/tweets_us_north_moderna_2021_05_31_to_05_23.csv']), ignore_index=True)\n",
    "# Make a new column with vaccine name\n",
    "moderna_north['vaccine'] = 'moderna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two csv files for week 3 and 4 of May\n",
    "janssen_north = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_us/us_north/JandJ/tweets_us_north_jandj_2021_05_22_to_05_16.csv', '../data/tweets_us/us_north/JandJ/tweets_us_north_jandj_2021_05_31_to_05_23.csv']), ignore_index=True)\n",
    "# Make a new column with vaccine name\n",
    "janssen_north['vaccine'] = 'johnson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two csv files for week 3 and 4 of May\n",
    "astra_north = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_us/us_north/astra/tweets_us_north_astra_2021_05_22_to_05_16.csv', '../data/tweets_us/us_north/astra/tweets_us_north_astra_2021_05_31_to_05_23.csv']), ignore_index=True)\n",
    "# Make a new column with vaccine name\n",
    "astra_north['vaccine'] = 'astrazeneca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes together\n",
    "tweets_us = pd.concat([astra_north, janssen_north, pfizer_north, moderna_north], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two csv files for week 3 and 4 of May\n",
    "all_north = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_us/us_north/tweets_us_north_all_2021_05_22_to_05_16.csv', '../data/tweets_us/us_north/tweets_us_north_all_2021_05_31_to_05_23.csv']), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15682, 24)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tweets ID that are in common with the 2 different searchs\n",
    "common_us_n = all_north.merge(tweets_us, how='inner', on=[\"Tweet Id\"])\n",
    "common_us_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armelleleguelte/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5099, 9)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tweets that are not in tweets_us\n",
    "all_us_n_no_com = all_fr[(~all_north[\"Tweet Id\"].isin(common[\"Tweet Id\"]))]\n",
    "all_us_n_no_com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with the respective vaccine for each tweet\n",
    "all_us_n_no_com.loc[all_us_n_no_com[\"Tweet Text\"].str.contains(\"pfizer|Pfizer\"), \"vaccine\"] = \"pfizer\"\n",
    "all_us_n_no_com.loc[all_us_n_no_com[\"Tweet Text\"].str.contains(\"moderna|Moderna\"), \"vaccine\"] = \"moderna\"\n",
    "all_us_n_no_com.loc[all_us_n_no_com[\"Tweet Text\"].str.contains(\"astrazeneca|Aztrazeneca\"), \"vaccine\"] = \"astrazeneca\"\n",
    "all_us_n_no_com.loc[all_us_n_no_com[\"Tweet Text\"].str.contains(\"johnson|Johnson\"), \"vaccine\"] = \"johnson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_us_n_no_com = all_us_n_no_com.dropna(subset=['vaccine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all_fr_no_com to vaccines_fr\n",
    "tweets_us_north = pd.concat([tweets_us, all_us_n_no_com], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and drop columns \n",
    "tweets_us_north = (tweets_us_north\n",
    "          .drop(columns = ['username', 'acctdesc', 'Friends Count','Account Verified', 'Favorite Count', 'hashtags'])\n",
    "          .rename(columns = {'Tweet Id': 'id', 'Tweet Date': 'date', 'Follower Count': 'follower_count', 'Retweets': 'retweets', 'Tweet Text': 'text'})\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"date\" object type to datetime\n",
    "tweets_us_north['date'] = tweets_us_north['date'].astype('Datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use eval to decode \"utf-8\" and remove \"\\n\"\n",
    "tweets_us_north[\"text\"] = tweets_us_north[\"text\"].map(lambda x: eval(x).decode(\"utf-8\").replace('\\n', ' '))\n",
    "# remove twitter handles\n",
    "tweets_us_north[\"text\"] = np.vectorize(remove_pattern)(tweets_us_north['text'], \"@[\\w]*\")\n",
    "# remove links\n",
    "tweets_us_north[\"text\"] = np.vectorize(remove_pattern)(tweets_us_north['text'], \"https://[\\w\\.\\\\/]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save file\n",
    "#tweets_us_north.to_csv('../data/tweets_us_north.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOUTHERN STATES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the files pfizer\n",
    "joined_files = os.path.join(\"../data/tweets_us/us_south/pfizer/\", \"tweets_us*.csv\")\n",
    "  \n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n",
    "  \n",
    "# Finally, the files are joined\n",
    "pfizer_south = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "\n",
    "# Make a new column with vaccine name\n",
    "pfizer_south['vaccine'] = 'pfizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the files moderna\n",
    "joined_files = os.path.join(\"../data/tweets_us/us_south/moderna/\", \"tweets_us*.csv\")\n",
    "  \n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n",
    "  \n",
    "# Finally, the files are joined\n",
    "moderna_south = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "\n",
    "# Make a new column with vaccine name\n",
    "moderna_south['vaccine'] = 'moderna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the files janssen\n",
    "joined_files = os.path.join(\"../data/tweets_us/us_south/JandJ/\", \"tweets_us*.csv\")\n",
    "  \n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n",
    "  \n",
    "# Finally, the files are joined\n",
    "janssen_south = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "\n",
    "# Make a new column with vaccine name\n",
    "janssen_south['vaccine'] = 'johnson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the files astra\n",
    "joined_files = os.path.join(\"../data/tweets_us/us_south/astra/\", \"tweets_us*.csv\")\n",
    "  \n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n",
    "  \n",
    "# Finally, the files are joined\n",
    "astra_south = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "\n",
    "# Make a new column with vaccine name\n",
    "astra_south['vaccine'] = 'astrazeneca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes together\n",
    "tweets_us_south = pd.concat([astra_north, janssen_north, pfizer_north, moderna_north], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two csv files for week 3 and 4 of May\n",
    "all_south = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_us/us_south/tweets_us_all_south_2021_05_22_to_05_16.csv', '../data/tweets_us/us_south/tweets_us_all_south_2021_05_31_to_05_23.csv']), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least one tweet was scraped wrongly and the values are not in the correct rows, I decide to drop it\n",
    "all_south = all_south[all_south[\"Tweet Id\"] != 'He/Him']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with no text ( probably other tweets not scrapped properly)\n",
    "all_south = all_south.dropna(subset=['Tweet Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armelleleguelte/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1599: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = infer_fill_value(value)\n",
      "/Users/armelleleguelte/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "# Add a column with the respective vaccine for each tweet\n",
    "all_south.loc[all_south[\"Tweet Text\"].str.contains(\"pfizer|Pfizer\"), \"vaccine\"] = \"pfizer\"\n",
    "all_south.loc[all_south[\"Tweet Text\"].str.contains(\"moderna|Moderna\"), \"vaccine\"] = \"moderna\"\n",
    "all_south.loc[all_south[\"Tweet Text\"].str.contains(\"astrazeneca|Aztrazeneca\"), \"vaccine\"] = \"astrazeneca\"\n",
    "all_south.loc[all_south[\"Tweet Text\"].str.contains(\"johnson|Johnson\"), \"vaccine\"] = \"johnson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_south = all_south.dropna(subset=['vaccine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all_fr_no_com to vaccines_fr\n",
    "tweets_us_south = pd.concat([tweets_us_south, all_south], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and drop columns \n",
    "tweets_us_south = (tweets_us_south\n",
    "          .drop(columns = ['username', 'acctdesc', 'Friends Count','Account Verified', 'Favorite Count', 'hashtags'])\n",
    "          .rename(columns = {'Tweet Id': 'id', 'Tweet Date': 'date', 'Follower Count': 'follower_count', 'Retweets': 'retweets', 'Tweet Text': 'text'})\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data types\n",
    "tweets_us_south['date'] = tweets_us_south['date'].astype('Datetime64')\n",
    "tweets_us_south['id'] = tweets_us_south['id'].astype('int64')\n",
    "tweets_us_south['follower_count'] = tweets_us_south['follower_count'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use eval to decode \"utf-8\" and remove \"\\n\"\n",
    "tweets_us_south[\"text\"] = tweets_us_south[\"text\"].map(lambda x: eval(x).decode(\"utf-8\").replace('\\n', ' '))\n",
    "# remove twitter handles\n",
    "tweets_us_south[\"text\"] = np.vectorize(remove_pattern)(tweets_us_south['text'], \"@[\\w]*\")\n",
    "# remove links\n",
    "tweets_us_south[\"text\"] = np.vectorize(remove_pattern)(tweets_us_south['text'], \"https://[\\w\\.\\\\/]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file\n",
    "tweets_us_south.to_csv('../data/tweets_us_south.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
