{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#merge and join files\n",
    "import glob\n",
    "import os\n",
    "#regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A- Clean kaggle data (only US and France):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_tweets = pd.read_csv('../data/tweets_us/kaggle_en_tweets/vaccination_all_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_tweets = kaggle_tweets.dropna(subset = ['user_location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Rename and drop columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_tweets = (kaggle_tweets\n",
    "          .drop(columns = ['user_created', 'user_favourites', 'source', 'favorites', 'is_retweet'])\n",
    "          .rename(columns = {'id': 'tweet_id'})\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Keep only tweets in France and USA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of user location (higher count on top)  \n",
    "#list_cities = kaggle_tweets.groupby('user_location')['user_location'].count().sort_values(ascending=False).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lists of places of interest for the study (France, south and north of USA)\n",
    "cities_fr = ['France', 'Paris, France', 'Nancy, France', 'Dammam, KSA | Tunisia | France', 'Alsace, France', 'Paris', 'La Belle Province', 'Bordeaux, France', 'Alpes-Maritimes, Provence-Alpe', '#CoronavirusMadeInPasteur', 'Rome, Italy - Beaulieu, France', 'Europe (usually Poland/France)', 'Paris, Limoges. France']\n",
    "cities_us_north = ['La Crescenta-Montrose, CA', 'San Francisco, CA', 'Los Angeles, CA', 'Rochester, NY', 'Cranbury, NJ', 'USAüá∫üá∏', 'America', 'Medford, OR', 'LI, NYC, NJ, PA, NE, USA', 'Chicago, IL', 'Seattle, WA',  'New Jersey, USA', 'Oregon, USA', 'California, USA', 'New York, NY', 'Cambridge, MA', 'New Orleans, LA', 'Michigan', 'Philadelphia, Pa.', 'North America', 'Troy, Michigan;  Serial ‚úàÔ∏è', 'Pennsylvania, USA', 'Saugerties, New York', 'Balboa Peninsula, SoCal USA', 'New York / New Jersey / Global', 'Minneapolis, MN', 'Vermont, US', 'Naperville, IL', 'Sacramento', 'New York, USA', 'Cherry Hill, NJ', 'NYC', 'San Jose, CA (USA)', 'Colorado Springs, Colorado ', 'Silicon Valley, CA', 'Washington, DC', 'D.C By Way Of BOSTON!üá≥üá¨', 'New York', 'Mequon, Wisconsin', 'Boston, MA', 'New York City, Westport, CT', 'Michigan, USA', 'US - IL - ID', 'Washington DC', 'CHICAGO!', 'NY', 'Brooklyn, NY', 'Los Angeles', 'Providence, RI', 'Boston', 'New York City', 'Rancho Cucamonga, CA', 'Syracuse, NY', 'New York & San Francisco', 'Philadelphia, PA', 'New Jersey', 'Buffalo, NY', 'Connecticut, USA', 'Rhode Island, USA', 'Northern Illinois', 'Rochester NY', 'Southern CA', 'Staten Island, New York', 'South Lake Tahoe, CA', 'Boston, MA.', 'washington', 'Paramus, NJ', 'Orange County, CA', 'Elmhurst, IL', 'Grand Rapids, MI', 'Long Beach, CA', 'Washington, D.C.', 'Northern California, USA', 'Princeton, NJ', 'Ventura, CA ', 'Washington D.C.', 'Rochester, MN', 'Denver, CO', 'Englewood, NJ', 'Katonah, NY', 'Chicago', 'Valencia,CA', 'Bellevue, WA', 'Minnesota, USA', 'Old Mystic, CT USA', 'San Francisco Bay Area', 'Santa Barbara, CA', 'Baltimore', 'Vermont, USA', 'Sunnyvale, CA', 'Bethesda, MD.', 'Queens, NY', 'Rochester MN', 'Social distancing in Manhattan', 'Morristown, NJ', 'Bronx, NY', 'DC Metro Area, USA', 'Maryland, USA', 'Stony Brook,  NY', 'Bryn Mawr, PA', 'Madison, WI', 'Coventry Rhode Island', 'Ann Arbor, ‚úã', 'Colorado, USA', 'Baltimore, MD', 'Pittsburgh, PA', 'Wisconsin, USA', 'minneapolis', 'Southern California', 'Northern California', 'Potomac  MD', 'Seaside, California, USA', 'Hartford, CT', 'Long Island   ', 'California', 'Paterson, NJ', 'NH', 'Manhattan, NYC', 'California Bay Area', 'VT', 'Stanford, CA', 'Duarte, CA', 'Los Angeles, California', 'Central California', 'Portland, ME', 'Philadelphia', 'Riverdale Park, MD ', 'Manhattan, NY', 'East Hampton, NY', 'Hollywood, CA, USA', 'South Cal', 'Ann Arbor, MI', 'Batavia, IL', 'Cambridge, MA, United States', 'Chicago-adjacent, IL', 'Metro North of Boston MA', 'New Haven, CT', 'Silver Spring, MD', 'West Hollywood, CA', 'So Cal', 'SoCal', 'California, United States', 'Sacramento, CA', 'Massachusetts, USA', 'Illinois, USA', 'Washington, USA', 'Fremont, CA, USA', 'New Jersey', 'Portland, OR', 'San Francisco', 'New York, New York', 'Santa Monica, CA', 'Connecticut', 'West Hartford, CT', 'Oakland, CA', 'Anaheim, CA', 'San Jose, CA', 'Buffalo, NY', 'Massachusetts', 'Los Angeles, California', 'Vermont, USA', 'Detroit, MI', 'Delaware, USA', 'Philadelphia', 'Boothbay Harbor, Maine', 'San Diego, California']\n",
    "cities_us_south = ['Texas', 'Jefferson City, MO',  'LA | OC ', 'Beaumont, Texas', 'Kentucky', 'Dallas, TX', 'Lakeland, FL', 'Palm Beach, FL', 'Tampa, FL', 'Okolona, KY', 'Houston, TX', 'Miami, FL', 'McAllen, TX', 'Texas, USA', 'Miami', 'Charleston, SC', 'Florida, USA', 'West Virginia, USA', 'Raleigh, NC', 'Raleigh, NC, USA', 'South Carolina, USA', 'Durham, NC', 'Alabama', 'Cincinnati, OH ', '11150A Asheville Hwy Inman, SC', 'Tucson, AZ', 'Albuquerque, NM', 'Nashville, TN', 'St Petersburg, FL', 'Austin, TX', 'Kansas', 'Roanoke, VA', 'Clinton, SC', 'Miami Beach, FL', 'San Antonio, Texas', 'Atlanta, GA', 'Odessa, TX', 'Chapel Hill, NC', 'Scottsdale, AZ', 'NEVADA', 'Richmond, VA', 'Atlanta, Georgia', 'Tysons Corner, Virginia', 'Phoenix, AZ', 'Greensboro, NC', 'Las Vegas, NV', 'Harlingen, TX', 'North Carolina, USA', 'Indianapolis, IN', 'Spring, TX', 'Idaho, USA', 'Athens, GA', 'Jacksonville, FL', 'Houston Texas Based', 'Jackson, MS', 'Covid Hell, AL', 'Kansas City, KS', 'Fort Worth', 'Charlottesville, VA', 'Kansas City, MO', 'Florida', 'Dayton, OH', 'Saint Louis, MO, USA', 'St Louis, MO', 'Knoxville, TN', 'Birmingham, AL', 'Columbia, MO', 'Morgantown, WV', 'TX', 'Allen, TX', 'Arkansas', 'Gainesville, FL', 'Smithville, TX', 'Augusta, GA', 'West Texas', 'Oklahoma City & Tulsa, OK', 'Oklahoma, USA', 'Gainesville, FL, USA', 'Arkansas, United States', 'Sugar Land, TX', 'Miami, Fl', 'Central FL', 'arkansas', 'Riverdale, UT', 'Nashville', 'Dallas', 'Houston', 'Reno, NV', 'Tampa Bay, FL', 'Charlotte, NC', 'Austin', 'Kansas City ', 'South Florida', 'Salt Lake City, UT', 'Alabama, USA', 'Ohio, USA', 'Virginia, USA', 'Orlando, FL', 'Las Vegas', 'Lufkin, TX', 'Arizona, USA', 'Cleveland, OH', 'Georgia, USA', 'Fort Lauderdale, FL', 'New York, Sarasota, FL', 'Columbus, OH', 'Louisville, KY', 'Tennessee, USA', 'Missouri, USA', 'Indiana, USA', 'Kentucky, USA', 'Oklahoma City, OK', 'North Carolina', 'New Mexico, USA', 'Rio Grande Valley, TX', 'Houston, Tx', 'Ohio', 'Cleveland, Ohio', 'Utah, USA', 'Indiana', 'Salt Lake City, Utah', 'Dallas, Texas', 'South Carolina', 'Columbus, Ohio', 'Corpus Christi, TX', 'Tampa, FL - United States', 'Atlanta', 'Indianapolis', 'Salt Lake City, UT', 'Alabama', 'TX', 'Fort Worth, TX', 'Jacksonville, N.C.', 'Tennessee', 'Arkansas, USA', 'Baton Rouge, LA', 'Arkansas', 'Jupiter, FL', 'Louisiana, USA', 'Tulsa, OK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dfs with the list of cities\n",
    "tweets_fr = kaggle_tweets[kaggle_tweets['user_location'].isin(cities_fr)]\n",
    "tweets_us_north = kaggle_tweets[kaggle_tweets['user_location'].isin(cities_us_north)]\n",
    "tweets_us_south = kaggle_tweets[kaggle_tweets['user_location'].isin(cities_us_south)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B- Clean data from Tweepy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- France:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge files to one file and add a column with the respective vaccines names(week 2-3):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging astrazeneca files\n",
    "astra_fr = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_fr/astra/tweets_fr_tweets_fr_aztra_2021_05_18_to_05_12.csv', '../data/tweets_fr/astra/tweets_fr_tweets_fr_astra_2021_05_19_to_05_13.csv']), ignore_index=True)\n",
    "astra_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column with vaccine name\n",
    "astra_fr['vaccine'] = 'astrazeneca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging J&J files\n",
    "jandj_fr = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_fr/JandJ/tweets_fr_jandj_2021_05_19_to_05_14.csv', '../data/tweets_fr/JandJ/tweets_fr_jandj_2021_05_18_to_05_11.csv']), ignore_index=True)\n",
    "jandj_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column with vaccine name\n",
    "jandj_fr['vaccine'] = 'johnson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging pfizer files\n",
    "pfizer_fr = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_fr/pfizer/tweets_fr_pfizer_2021_05_14_to_05_11.csv', '../data/tweets_fr/pfizer/tweets_fr_pfizer_2021_05_18_to_05_15.csv']), ignore_index=True)\n",
    "pfizer_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column with vaccine name\n",
    "pfizer_fr['vaccine'] = 'pfizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moderna\n",
    "moderna_fr = pd.read_csv('../data/tweets_fr/moderna/tweets_fr_moderna_2021_05_18_to_05_11.csv')\n",
    "moderna_fr['vaccine'] = 'moderna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes together\n",
    "vaccines_fr = pd.concat([astra_fr, jandj_fr, pfizer_fr, moderna_fr], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccines_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweets with a different search\n",
    "all_fr = pd.read_csv('../data/tweets_fr/tweets_fr_all_2021_05_19_to_05_11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = [\"Tweet Id\", \"Tweet Date\", \"location\", \"Follower Count\", \"Account Verified\", \"Favorite Count\", \"Retweets\", \"Tweet Text\", \"hashtags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tweets ID that are in common with the 2 different searchs\n",
    "common = all_fr.merge(vaccines_fr, how='inner', on=[\"Tweet Id\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tweets that are not in vaccine_fr\n",
    "all_fr_no_com = all_fr[(~all_fr[\"Tweet Id\"].isin(common[\"Tweet Id\"]))]\n",
    "all_fr_no_com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a column with the respective vaccine for each tweet\n",
    "all_fr_no_com.loc[all_fr_no_com[\"Tweet Text\"].str.contains(\"pfizer|Pfizer\"), \"vaccine\"] = \"pfizer\"\n",
    "all_fr_no_com.loc[all_fr_no_com[\"Tweet Text\"].str.contains(\"moderna|Moderna\"), \"vaccine\"] = \"moderna\"\n",
    "all_fr_no_com.loc[all_fr_no_com[\"Tweet Text\"].str.contains(\"astrazeneca|Aztrazeneca\"), \"vaccine\"] = \"astrazeneca\"\n",
    "all_fr_no_com.loc[all_fr_no_com[\"Tweet Text\"].str.contains(\"johnson|Johnson\"), \"vaccine\"] = \"johnson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in column \"vaccine\"\n",
    "all_fr_no_com = all_fr_no_com.dropna(subset=[9], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all_fr_no_com to vaccines_fr\n",
    "vaccines_fr = pd.concat([vaccines_fr, all_fr_no_com], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and drop columns \n",
    "vaccines_fr = (vaccines_fr\n",
    "          .drop(columns = ['Account Verified', 'Favorite Count', 'hashtags'])\n",
    "          .rename(columns = {'Tweet Id': 'id', 'Tweet Date': 'date', 'Follower Count': 'follower_count', 'Retweets': 'retweets', 'Tweet Text': 'text'})\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"date\" object type to datetime\n",
    "vaccines_fr['date'] = vaccines_fr['date'].astype('Datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode 'utf-8' characters with real characters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use eval to decode \"utf-8\" and remove \"\\n\"\n",
    "vaccines_fr[\"text\"] = vaccines_fr[\"text\"].map(lambda x: eval(x).decode(\"utf-8\").replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Twitter Handles (@user) and links:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_text, pattern):\n",
    "    reg = re.findall(pattern, input_text)\n",
    "    for x in reg:\n",
    "        input_text = re.sub(x, '', input_text)\n",
    "        \n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter handles\n",
    "vaccines_fr[\"text\"] = np.vectorize(remove_pattern)(vaccines_fr['text'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links\n",
    "vaccines_fr[\"text\"] = np.vectorize(remove_pattern)(vaccines_fr['text'], \"https://[\\w\\.\\\\/]*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save \"vaccine_fr\" as a csv file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vaccines_fr.to_csv('../data/vaccines_fr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge files to one file and add a column with the respective vaccines names(week 4):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfizer_fr_week4 = pd.read_csv('../data/tweets_fr/pfizer/tweets_fr_pfizer_2021_05_29_to_05_22.csv')\n",
    "pfizer_fr_week4['vaccine'] = 'pfizer'\n",
    "moderna_fr_week4 = pd.read_csv('../data/tweets_fr/moderna/tweets_fr_moderna_2021_05_29_to_05_22.csv')\n",
    "moderna_fr_week4['vaccine'] = 'moderna'\n",
    "astra_fr_week4 = pd.read_csv('../data/tweets_fr/astra/tweets_fr_astra_2021_05_29_to_05_22.csv')\n",
    "astra_fr_week4['vaccine'] = 'astrazeneca'\n",
    "janssen_fr_week4 = pd.read_csv('../data/tweets_fr/JandJ/tweets_fr_jandj_2021_05_29_to_05_22.csv')\n",
    "janssen_fr_week4['vaccine'] = 'johnson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all_fr_no_com to vaccines_fr\n",
    "vaccines_fr_week4 = pd.concat([pfizer_fr_week4, moderna_fr_week4, astra_fr_week4, janssen_fr_week4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and drop columns \n",
    "vaccines_fr_week4 = (vaccines_fr_week4\n",
    "          .drop(columns = ['username', 'acctdesc', 'Friends Count','Account Verified', 'Favorite Count', 'hashtags'])\n",
    "          .rename(columns = {'Tweet Id': 'id', 'Tweet Date': 'date', 'Follower Count': 'follower_count', 'Retweets': 'retweets', 'Tweet Text': 'text'})\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"date\" object type to datetime\n",
    "vaccines_fr_week4['date'] = vaccines_fr_week4['date'].astype('Datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use eval to decode \"utf-8\" and remove \"\\n\"\n",
    "vaccines_fr_week4[\"text\"] = vaccines_fr_week4[\"text\"].map(lambda x: eval(x).decode(\"utf-8\").replace('\\n', ' '))\n",
    "# remove twitter handles\n",
    "vaccines_fr_week4[\"text\"] = np.vectorize(remove_pattern)(vaccines_fr_week4['text'], \"@[\\w]*\")\n",
    "# remove links\n",
    "vaccines_fr_week4[\"text\"] = np.vectorize(remove_pattern)(vaccines_fr_week4['text'], \"https://[\\w\\.\\\\/]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save file\n",
    "#vaccines_fr_week4.to_csv('../data/vaccines_fr_week4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- USA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NORTHERN STATES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two csv files for week 3 and 4 of May\n",
    "pfizer_north = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_us/us_north/pfizer/tweets_us_north_pfizer_2021_05_22_to_05_16.csv', '../data/tweets_us/us_north/pfizer/tweets_us_north_pfizer_2021_05_31_to_05_23.csv']), ignore_index=True)\n",
    "# Make a new column with vaccine name\n",
    "pfizer_north['vaccine'] = 'pfizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two csv files for week 3 and 4 of May\n",
    "moderna_north = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_us/us_north/moderna/tweets_us_north_moderna_2021_05_22_to_05_16.csv', '../data/tweets_us/us_north/moderna/tweets_us_north_moderna_2021_05_31_to_05_23.csv']), ignore_index=True)\n",
    "# Make a new column with vaccine name\n",
    "moderna_north['vaccine'] = 'moderna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two csv files for week 3 and 4 of May\n",
    "janssen_north = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_us/us_north/JandJ/tweets_us_north_jandj_2021_05_22_to_05_16.csv', '../data/tweets_us/us_north/JandJ/tweets_us_north_jandj_2021_05_31_to_05_23.csv']), ignore_index=True)\n",
    "# Make a new column with vaccine name\n",
    "janssen_north['vaccine'] = 'johnson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two csv files for week 3 and 4 of May\n",
    "astra_north = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_us/us_north/astra/tweets_us_north_astra_2021_05_22_to_05_16.csv', '../data/tweets_us/us_north/astra/tweets_us_north_astra_2021_05_31_to_05_23.csv']), ignore_index=True)\n",
    "# Make a new column with vaccine name\n",
    "astra_north['vaccine'] = 'astrazeneca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes together\n",
    "tweets_us = pd.concat([astra_north, janssen_north, pfizer_north, moderna_north], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two csv files for week 3 and 4 of May\n",
    "all_north = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_us/us_north/tweets_us_north_all_2021_05_22_to_05_16.csv', '../data/tweets_us/us_north/tweets_us_north_all_2021_05_31_to_05_23.csv']), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tweets ID that are in common with the 2 different searchs\n",
    "common_us_n = all_north.merge(tweets_us, how='inner', on=[\"Tweet Id\"])\n",
    "common_us_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tweets that are not in tweets_us\n",
    "all_us_n_no_com = all_fr[(~all_north[\"Tweet Id\"].isin(common[\"Tweet Id\"]))]\n",
    "all_us_n_no_com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with the respective vaccine for each tweet\n",
    "all_us_n_no_com.loc[all_us_n_no_com[\"Tweet Text\"].str.contains(\"pfizer|Pfizer\"), \"vaccine\"] = \"pfizer\"\n",
    "all_us_n_no_com.loc[all_us_n_no_com[\"Tweet Text\"].str.contains(\"moderna|Moderna\"), \"vaccine\"] = \"moderna\"\n",
    "all_us_n_no_com.loc[all_us_n_no_com[\"Tweet Text\"].str.contains(\"astrazeneca|Aztrazeneca\"), \"vaccine\"] = \"astrazeneca\"\n",
    "all_us_n_no_com.loc[all_us_n_no_com[\"Tweet Text\"].str.contains(\"johnson|Johnson\"), \"vaccine\"] = \"johnson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_us_n_no_com = all_us_n_no_com.dropna(subset=['vaccine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all_fr_no_com to vaccines_fr\n",
    "tweets_us_north = pd.concat([tweets_us, all_us_n_no_com], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and drop columns \n",
    "tweets_us_north = (tweets_us_north\n",
    "          .drop(columns = ['username', 'acctdesc', 'Friends Count','Account Verified', 'Favorite Count', 'hashtags'])\n",
    "          .rename(columns = {'Tweet Id': 'id', 'Tweet Date': 'date', 'Follower Count': 'follower_count', 'Retweets': 'retweets', 'Tweet Text': 'text'})\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"date\" object type to datetime\n",
    "tweets_us_north['date'] = tweets_us_north['date'].astype('Datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use eval to decode \"utf-8\" and remove \"\\n\"\n",
    "tweets_us_north[\"text\"] = tweets_us_north[\"text\"].map(lambda x: eval(x).decode(\"utf-8\").replace('\\n', ' '))\n",
    "# remove twitter handles\n",
    "tweets_us_north[\"text\"] = np.vectorize(remove_pattern)(tweets_us_north['text'], \"@[\\w]*\")\n",
    "# remove links\n",
    "tweets_us_north[\"text\"] = np.vectorize(remove_pattern)(tweets_us_north['text'], \"https://[\\w\\.\\\\/]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save file\n",
    "#tweets_us_north.to_csv('../data/tweets_us_north.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOUTHERN STATES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the files pfizer\n",
    "joined_files = os.path.join(\"../data/tweets_us/us_south/pfizer/\", \"tweets_us*.csv\")\n",
    "  \n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n",
    "  \n",
    "# Finally, the files are joined\n",
    "pfizer_south = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "\n",
    "# Make a new column with vaccine name\n",
    "pfizer_south['vaccine'] = 'pfizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the files moderna\n",
    "joined_files = os.path.join(\"../data/tweets_us/us_south/moderna/\", \"tweets_us*.csv\")\n",
    "  \n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n",
    "  \n",
    "# Finally, the files are joined\n",
    "moderna_south = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "\n",
    "# Make a new column with vaccine name\n",
    "moderna_south['vaccine'] = 'moderna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the files janssen\n",
    "joined_files = os.path.join(\"../data/tweets_us/us_south/JandJ/\", \"tweets_us*.csv\")\n",
    "  \n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n",
    "  \n",
    "# Finally, the files are joined\n",
    "janssen_south = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "\n",
    "# Make a new column with vaccine name\n",
    "janssen_south['vaccine'] = 'johnson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the files astra\n",
    "joined_files = os.path.join(\"../data/tweets_us/us_south/astra/\", \"tweets_us*.csv\")\n",
    "  \n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n",
    "  \n",
    "# Finally, the files are joined\n",
    "astra_south = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "\n",
    "# Make a new column with vaccine name\n",
    "astra_south['vaccine'] = 'astrazeneca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes together\n",
    "tweets_us_south = pd.concat([astra_north, janssen_north, pfizer_north, moderna_north], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two csv files for week 3 and 4 of May\n",
    "all_south = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweets_us/us_south/tweets_us_all_south_2021_05_22_to_05_16.csv', '../data/tweets_us/us_south/tweets_us_all_south_2021_05_31_to_05_23.csv']), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least one tweet was scraped wrongly and the values are not in the correct rows, I decide to drop it\n",
    "all_south = all_south[all_south[\"Tweet Id\"] != 'He/Him']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with no text ( probably other tweets not scrapped properly)\n",
    "all_south = all_south.dropna(subset=['Tweet Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with the respective vaccine for each tweet\n",
    "all_south.loc[all_south[\"Tweet Text\"].str.contains(\"pfizer|Pfizer\"), \"vaccine\"] = \"pfizer\"\n",
    "all_south.loc[all_south[\"Tweet Text\"].str.contains(\"moderna|Moderna\"), \"vaccine\"] = \"moderna\"\n",
    "all_south.loc[all_south[\"Tweet Text\"].str.contains(\"astrazeneca|Aztrazeneca\"), \"vaccine\"] = \"astrazeneca\"\n",
    "all_south.loc[all_south[\"Tweet Text\"].str.contains(\"johnson|Johnson\"), \"vaccine\"] = \"johnson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_south = all_south.dropna(subset=['vaccine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all_fr_no_com to vaccines_fr\n",
    "tweets_us_south = pd.concat([tweets_us_south, all_south], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and drop columns \n",
    "tweets_us_south = (tweets_us_south\n",
    "          .drop(columns = ['username', 'acctdesc', 'Friends Count','Account Verified', 'Favorite Count', 'hashtags'])\n",
    "          .rename(columns = {'Tweet Id': 'id', 'Tweet Date': 'date', 'Follower Count': 'follower_count', 'Retweets': 'retweets', 'Tweet Text': 'text'})\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data types\n",
    "tweets_us_south['date'] = tweets_us_south['date'].astype('Datetime64')\n",
    "tweets_us_south['id'] = tweets_us_south['id'].astype('int64')\n",
    "tweets_us_south['follower_count'] = tweets_us_south['follower_count'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use eval to decode \"utf-8\" and remove \"\\n\"\n",
    "tweets_us_south[\"text\"] = tweets_us_south[\"text\"].map(lambda x: eval(x).decode(\"utf-8\").replace('\\n', ' '))\n",
    "# remove twitter handles\n",
    "tweets_us_south[\"text\"] = np.vectorize(remove_pattern)(tweets_us_south['text'], \"@[\\w]*\")\n",
    "# remove links\n",
    "tweets_us_south[\"text\"] = np.vectorize(remove_pattern)(tweets_us_south['text'], \"https://[\\w\\.\\\\/]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file\n",
    "#tweets_us_south.to_csv('../data/tweets_us_south.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C- Cleaning data from Tweepy round 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FRANCE: Janssen tweets have a lot of noise due to a lot of celebrities having a last name \"Johnson\" like Boris Johnson for example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Janssen search using tweepy\n",
    "janssen_fr = pd.read_csv('../data/tweets_fr/tweets_fr_jandj_2021_06_03_to_05_27.csv')\n",
    "janssen_fr['vaccine'] = 'johnson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and drop columns \n",
    "janssen_fr = (janssen_fr\n",
    "          .drop(columns = ['username', 'acctdesc', 'Friends Count','Account Verified', 'Favorite Count', 'hashtags'])\n",
    "          .rename(columns = {'Tweet Id': 'id', 'Tweet Date': 'date', 'Follower Count': 'follower_count', 'Retweets': 'retweets', 'Tweet Text': 'text'})\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"date\" object type to datetime\n",
    "janssen_fr['date'] = janssen_fr['date'].astype('Datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use eval to decode \"utf-8\" and remove \"\\n\"\n",
    "janssen_fr[\"text\"] = janssen_fr[\"text\"].map(lambda x: eval(x).decode(\"utf-8\").replace('\\n', ' '))\n",
    "# remove twitter handles\n",
    "janssen_fr[\"text\"] = np.vectorize(remove_pattern)(janssen_fr['text'], \"@[\\w]*\")\n",
    "# remove links\n",
    "janssen_fr[\"text\"] = np.vectorize(remove_pattern)(janssen_fr['text'], \"https://[\\w\\.\\\\/]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file\n",
    "#janssen_fr.to_csv('../data/janssen_fr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging files\n",
    "tweets_fr = pd.concat(\n",
    "    map(pd.read_csv, ['../data/tweet_fr/tweets_fr_week_2_translated.csv', '../data/tweet_fr/tweets_fr_week4_translated.csv']), ignore_index=True)\n",
    "tweets_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ride of johnson vaccine tweets because the majority are about Boris Johnson (tweepy search wrong)\n",
    "tweets_fr[\"boris\"] = tweets_fr['text'].str.contains(\"Boris|boris\")\n",
    "tweets_fr = tweets_fr[tweets_fr.boris == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_fr = tweets_fr.drop(columns = [\"boris\", \"translated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes together\n",
    "tweets_fr = pd.concat([tweets_fr, janssen_fr], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_fr = tweets_fr.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of user location (higher count on top)  \n",
    "#list_cities = tweets_fr.groupby('location')['location'].count().sort_values(ascending=False).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets location with more than 10 tweets not in France\n",
    "not_france = ['Bruxelles', 'Suisse', 'Brussels', 'Gen√®ve, Suisse', 'Bruxelles, Belgique', 'Luxembourg', 'London, England', 'Li√®ge, Belgique', 'Uccle, Belgique', 'Z√ºrich', 'Geneva, Switzerland', 'Saitama\\n\\n„Åï„ÅÑ„Åü„ÅæÂ∏Ç\\n\\n', 'BRUSSELS', 'Namur, Belgium']\n",
    "tweets_fr = tweets_fr[~tweets_fr['location'].isin(not_france)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file\n",
    "#tweets_fr.to_csv('../data/tweets_fr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_fr_trans = pd.read_csv('../data/tweets_fr_translated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is tweets texts that are duplicates with different tweets id!\n",
    "tweets_fr_trans.drop_duplicates('text', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets about campaign of vaccination (about 11% of the data)\n",
    "tweets_fr_trans['campaign'] = tweets_fr_trans['text'].str.contains(\"doses sont disponibles|disponibles aujourd'hui|disponible aujourd'hui|Rdv dispo|disponibles demain|disponible demain|doses sont disponibles|cr√©neau disponible|cr√©neaux disponibles|Centre|Centres|Alpexo and CHU\")\n",
    "tweets_fr_trans = tweets_fr_trans[tweets_fr_trans.campaign == False]\n",
    "tweets_fr_trans = tweets_fr_trans.drop(columns = [\"campaign\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file\n",
    "#tweets_fr_trans.to_csv('../data/tweets_fr_translated_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USA NORTH: Clean location (several tweets are not in NE region):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_north = pd.read_csv('../data/tweets_us_north.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of user location (higher count on top)  \n",
    "#list_cities_us_n = tweets_us_north.groupby('location')['location'].count().sort_values(ascending=False).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tweets in location = France\n",
    "tweets_us_north[\"france\"] = tweets_us_north['location'].str.contains(\"France|france|FRANCE|Suisse|VA|California|Florida|Bruxelles\")\n",
    "tweets_us_north = tweets_us_north[tweets_us_north.france == False]\n",
    "tweets_us_north = tweets_us_north.drop(columns = [\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tweets in french\n",
    "tweets_us_north[\"french_tweets\"] = tweets_us_north['text'].str.contains(\"pour|fait|mais|avoir|faire|demain|semaine|aussi|bien|vous|contre|suis|jour|rien\")\n",
    "tweets_us_north = tweets_us_north[tweets_us_north.french_tweets == False]\n",
    "tweets_us_north = tweets_us_north.drop(columns = [\"french_tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_us_n = ['Paris', 'Maryland, USA', 'Washington, USA', 'Baltimore, MD', 'Georgia, USA', 'Marseille', 'Meurthe-et-Moselle', 'Jamaica', 'Maryland', 'New Mexico, USA', 'Essex', 'Nantes', 'Narnia', 'Georgia', 'Strasbourg', 'Lyon', 'Washington', 'Rennes', 'Clermont Ferrand', 'Dorset', 'Lancashire', 'Middle Earth', 'Silver Spring, MD','Baltimore', 'Yorkshire', 'South Wales' , 'Arcturus', 'Paris, Ile-de-France', 'Baltimore, Maryland', 'Idaho, USA', 'Provence-Alpes-C√¥te d\\'Azur', 'Rouen', 'paris', 'Lille', 'Luxembourg', 'London', 'Bordeaux', 'North Wales', 'Toulouse', 'Montpellier', 'Nord-Pas-de-Calais, France', 'FRANCE', 'Paris', 'Idaho', 'Nice', 'Grenoble', 'Brussels', 'Arlington, Virginia', 'R√©gion Parisienne', 'california', 'Bretagne', 'Metz', 'London, England', 'Beaulieu-sur-Mer', 'Dijon', 'Haute-Vienne', 'East Texas', 'Marseille']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_north = tweets_us_north[~tweets_us_north['location'].isin(not_us_n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are tweets texts that are duplicates with different tweets id!\n",
    "tweets_us_north.drop_duplicates('text', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets about campaign of vaccination (about 15% of the data)\n",
    "tweets_us_north['campaign'] = tweets_us_north['text'].str.contains(\"pendant|dans|Vaccine appointments are available at\")\n",
    "tweets_us_north = tweets_us_north[tweets_us_north.campaign == False]\n",
    "tweets_us_north = tweets_us_north.drop(columns = [\"campaign\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file\n",
    "#tweets_us_north.to_csv('../data/tweets_us_north-clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**US SOUTH: clean location (some locations are outside of SE region):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_south = pd.read_csv('../data/tweets_us_south.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of user location (higher count on top)  \n",
    "list_cities_us_s = tweets_us_south.groupby('location')['location'].count().sort_values(ascending=False).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location outside of SE region\n",
    "tweets_us_south[\"not_s\"] = tweets_us_south['location'].str.contains(\"Northumberland|Massachusetts|Pennsylvania|VA|Virginia|Texas|DE|NY|DC|Ireland|MA|PA|CT|RI|NJ|IL|NH|Washington|California|california|New York|TX|NYC|MD\")\n",
    "tweets_us_south = tweets_us_south[tweets_us_south.not_s == False]\n",
    "tweets_us_south = tweets_us_south.drop(columns = [\"not_s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets location with more than 10 tweets not in SE\n",
    "not_us_s = ['New Jersey, USA','Pennsylvania, USA', 'Massachusetts, USA', 'Maryland, USA', 'Arizona, USA', 'Connecticut, USA', 'Illinois, USA', 'Minnesota, USA', 'Boston', 'Pennsylvania', 'Lebanon', 'Massachusetts', 'Rhode Island', 'Switzerland', 'Maine, USA', 'Wood River, Illinois 62095', 'NJ', 'Netherlands', 'Jamaica', 'Minnesota', 'Philadelphia', 'North America', 'Maryland', 'Connecticut', 'Houston, Texas', 'Illinois', 'Israel', 'Denmark', 'Utah, USA', 'New Mexico, USA', 'Philadelphia, Pa', 'Utah', 'Maine', 'Sweden', 'Essex', 'Nevada, USA', 'The Moon', 'Narnia', 'Brooklyn', 'nyc', 'Jersey City, NJ', 'Hungary', 'Hindustan', 'Jersey', 'Dorset', 'Rhode Island, USA', 'Lancashire', 'Middle Earth', 'Europa', 'Yorkshire', 'South Wales', 'Arcturus', 'Dallas, Texas', 'Idaho, USA', 'New Hampshire', 'Wilmington, DE', 'Philly', 'Cheshire', 'North Wales', 'Sparta, NJ', 'ireland', 'Idaho', 'Texas, USA', 'Tyler, Texas', 'Maryland', 'new york', 'Dallas', 'Middlebury, VT', 'Massachusetts', 'Dallas County', 'Newfoundland', 'District of Columbia, USA', 'West End, District of Columbia', 'Providence, Rhode Island', 'boston', 'Nevada', 'North', 'Venus', 'Internet', 'Hollywood Hills', 'India', 'Northeast U.S.', 'Oxfordshire', 'E', 'Nevada', 'IRELAND', 'brooklyn', 'Boston, Ma', 'Switzerland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_south = tweets_us_south[~tweets_us_south['location'].isin(not_us_s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is tweets texts that are duplicates with different tweets id!\n",
    "tweets_us_south.drop_duplicates('text', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets about campaign of vaccination (about 1.5% of the data)\n",
    "tweets_us_south['campaign'] = tweets_us_south['text'].str.contains(\"appointments|clinic|clinics|center|CALL NOW|No appointment needed|to make an appointment|Schedule an appointment|Schedule your appointment|schedule your appointment|no appointment required|No appointment is necessary|patient|no appointment necessary|Center|Mass Vaccination Sites|Find an appointment|Duke Health|UNC Health|receive a free bus pass|Please text|Walk-ins are welcome|No appointment necessary|Walk in\")\n",
    "tweets_us_south = tweets_us_south[tweets_us_south.campaign == False]\n",
    "tweets_us_south = tweets_us_south.drop(columns = [\"campaign\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 8% are about campaign for vaccination \n",
    "tweets_us_south['campaign'] = tweets_us_south['text'].str.contains(\"appointment\")\n",
    "test= tweets_us_south[tweets_us_south.campaign == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_south.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file\n",
    "tweets_us_south.to_csv('../data/tweets_us_south_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
