{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#Sentiment analysis\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
    "\n",
    "#emotion\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import chart_studio\n",
    "import chart_studio.plotly as py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'xxx' # your username\n",
    "api_key = 'xxx' # your api key - go to profile > settings > regenerate key\n",
    "chart_studio.tools.set_credentials_file(username=username, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 300\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A- Sentiment analysis of french and US tweets per vaccine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files\n",
    "tweets_fr = pd.read_csv('../data/tweets_fr_translated_clean.csv')\n",
    "tweets_us_north = pd.read_csv('../data/tweets_us_north-clean.csv')\n",
    "tweets_us_south = pd.read_csv('../data/tweets_us_south_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Perform sentiment analysis classification by using pretrained model â€œdistilbert-base-uncased-finetuned-sst-2-englishâ€ (ðŸ¤—) -- Classify the Tweets as positive or negative::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply classifier to the french tweets translated in english\n",
    "tweets_fr[\"sen_ana\"] = tweets_fr[\"translated_text\"].apply(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_north[\"sen_ana\"] = tweets_us_north[\"text\"].apply(classifier)\n",
    "tweets_us_south[\"sen_ana\"] = tweets_us_south[\"text\"].apply(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make functions to split result into two columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_label(lb):\n",
    "    label = re.findall(r\"[A-Z]+\", lb)[0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_score(sc):\n",
    "    score = re.findall(r\"\\d+\\.\\d+\", sc)[0]\n",
    "    return score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_fr[\"score\"] = tweets_fr[\"sen_ana\"].apply(str).apply(split_score)\n",
    "tweets_fr[\"label\"] = tweets_fr[\"sen_ana\"].apply(str).apply(split_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_north[\"score\"] = tweets_us_north[\"sen_ana\"].apply(str).apply(split_score)\n",
    "tweets_us_north[\"label\"] = tweets_us_north[\"sen_ana\"].apply(str).apply(split_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_south[\"score\"] = tweets_us_south[\"sen_ana\"].apply(str).apply(split_score)\n",
    "tweets_us_south[\"label\"] = tweets_us_south[\"sen_ana\"].apply(str).apply(split_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column score from object to float\n",
    "tweets_fr['score'] = tweets_fr['score'].astype('float64')\n",
    "tweets_us_north['score'] = tweets_us_north['score'].astype('float64')\n",
    "tweets_us_south['score'] = tweets_us_south['score'].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace names to be the same across notebooks\n",
    "tweets_fr[\"vaccine\"] = tweets_fr[\"vaccine\"].replace([\"pfizer\", \"moderna\", \"astrazeneca\", \"johnson\"], [\"Pfizer\", \"Moderna\", \"AstraZenca\", \"Johnson&Johnson\"])\n",
    "tweets_fr[\"label\"] = tweets_fr[\"label\"].replace([\"NEGATIVE\", \"POSITIVE\"], [\"Negative\", \"Positive\"])\n",
    "\n",
    "tweets_us_north[\"vaccine\"] = tweets_us_north[\"vaccine\"].replace([\"pfizer\", \"moderna\", \"astrazeneca\", \"johnson\"], [\"Pfizer\", \"Moderna\", \"AstraZenca\", \"Johnson&Johnson\"])\n",
    "tweets_us_north[\"label\"] = tweets_us_north[\"label\"].replace([\"NEGATIVE\", \"POSITIVE\"], [\"Negative\", \"Positive\"])\n",
    "\n",
    "tweets_us_south[\"vaccine\"] = tweets_us_south[\"vaccine\"].replace([\"pfizer\", \"moderna\", \"astrazeneca\", \"johnson\"], [\"Pfizer\", \"Moderna\", \"AstraZenca\", \"Johnson&Johnson\"])\n",
    "tweets_us_south[\"label\"] = tweets_us_south[\"label\"].replace([\"NEGATIVE\", \"POSITIVE\"], [\"Negative\", \"Positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_fr.to_csv('../data/tweet_fr_sa.csv', index=False)\n",
    "#tweets_us_north.to_csv('../data/tweets_us_north_sa.csv', index=False)\n",
    "#tweets_us_south.to_csv('../data/tweets_us_south_sa.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make violin plots:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open files with sentiment analysis possitive or negative\n",
    "tweets_fr = pd.read_csv('../data/tweet_fr_sa.csv')\n",
    "tweets_us_ne = pd.read_csv('../data/tweets_us_north_sa.csv')\n",
    "tweets_us_se = pd.read_csv('../data/tweets_us_south_sa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# France\n",
    "fig1 = px.violin(tweets_fr, x='vaccine', y='score', color ='label', title= 'Sentiments analysis of French tweets toward COVID-19 vaccines',\n",
    "            labels={\n",
    "                 \"score\": \"Score\",\n",
    "                 \"vaccine\": \"\",\n",
    "                 \"label\": \"Sentiment\"\n",
    "             },\n",
    "            color_discrete_map={ # replaces default color mapping by value\n",
    "                \"Negative\": \"steelblue\", \"Positive\": \"lightsteelblue\"\n",
    "            },\n",
    "                        category_orders={\"vaccine\": [\"Pfizer\", \"Moderna\", \"AstraZenca\", \"Johnson&Johnson\"]\n",
    "                        },\n",
    "             template=\"simple_white\"\n",
    "            )\n",
    "fig1.update_yaxes(showgrid=True, showline=False, tickwidth=0, tickcolor='white')\n",
    "fig1.update_xaxes(showline=True, zeroline=True)\n",
    "            \n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#py.plot(fig1, filename = 'sentiment_analysis_france', auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US NE\n",
    "fig2 = px.violin(tweets_us_ne, x='vaccine', y='score', color ='label',title= 'Sentiments analysis of US Northeast Tweets toward COVID-19 vaccines',\n",
    "            labels={\n",
    "                 \"score\": \"Score\",\n",
    "                 \"vaccine\": \"\",\n",
    "                 \"label\": \"Sentiment\"\n",
    "             },\n",
    "            color_discrete_map={ # replaces default color mapping by value\n",
    "                \"Negative\": \"steelblue\", \"Positive\": \"lightsteelblue\"\n",
    "            },\n",
    "                        category_orders={\"vaccine\": [\"Pfizer\", \"Moderna\", \"AstraZenca\", \"Johnson&Johnson\"]\n",
    "                        },\n",
    "             template=\"simple_white\"\n",
    "            )\n",
    "fig1.update_yaxes(showgrid=True, showline=False, tickwidth=0, tickcolor='white')\n",
    "fig1.update_xaxes(showline=True, zeroline=True)\n",
    "            \n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#py.plot(fig2, filename = 'sentiment_analysis_us_north', auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US SE\n",
    "fig3 = px.violin(tweets_us_se, x='vaccine', y='score', color ='label',title= 'Sentiments analysis of US Southeast Tweets toward COVID-19 vaccines',\n",
    "            labels={\n",
    "                 \"score\": \"Score\",\n",
    "                 \"vaccine\": \"\",\n",
    "                 \"label\": \"Sentiment\"\n",
    "             },\n",
    "            color_discrete_map={ # replaces default color mapping by value\n",
    "                \"Negative\": \"steelblue\", \"Positive\": \"lightsteelblue\"\n",
    "            },\n",
    "                        category_orders={\"vaccine\": [\"Pfizer\", \"Moderna\", \"AstraZenca\", \"Johnson&Johnson\"]\n",
    "                        },\n",
    "             template=\"simple_white\"\n",
    "            )\n",
    "fig1.update_yaxes(showgrid=True, showline=False, tickwidth=0, tickcolor='white')\n",
    "fig1.update_xaxes(showline=True, zeroline=True)\n",
    "            \n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#py.plot(fig3, filename = 'sentiment_analysis_us_south', auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Perform sentiment analysis classification by using pretrained model â€œcardifnlp/bertweet-base-sentiment â€ (ðŸ¤—) -- Classify the Tweets as positive, negative or neutral:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FRANCE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento import SentimentAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the analysis on english Tweets\n",
    "analyzer = SentimentAnalyzer(lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define function to get sentiments\n",
    "def sentiment_analysis(df):\n",
    "    sen_ana=[]\n",
    "    for index, row in df.iterrows():\n",
    "        #doesn't work without the try (potential problems with few tweets)\n",
    "        try:\n",
    "            texts = row[\"translated_text\"]\n",
    "            tweet_id = row[\"id\"]\n",
    "            result = analyzer.predict(texts)\n",
    "            x = [tweet_id, result]\n",
    "            sen_ana.append(x)\n",
    "        except:\n",
    "            pass\n",
    "    result_df = pd.DataFrame(sen_ana, columns=[\"id\", \"sent_score\"])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function \n",
    "tweets_fr_bert = sentiment_analysis(tweets_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean results using regex:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_output(op):\n",
    "    output = re.findall(r\"\\=\\w+\", op)[0]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neutral(ne):\n",
    "    neutral = re.findall(r\"NEU:\\W\\d\\.\\d+\", ne)[0]\n",
    "    return neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positive(po):\n",
    "    positive = re.findall(r\"POS:\\W\\d\\.\\d+\", po)[0]\n",
    "    return positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_negative(ng):\n",
    "    negative = re.findall(r\"NEG:\\W\\d\\.\\d+\", ng)[0]\n",
    "    return negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply functions\n",
    "tweets_fr_bert[\"output\"] = tweets_fr_bert[\"sent_score\"].apply(str).apply(find_output)\n",
    "tweets_fr_bert[\"neutral\"] = tweets_fr_bert[\"sent_score\"].apply(str).apply(find_neutral)\n",
    "tweets_fr_bert[\"positive\"] = tweets_fr_bert[\"sent_score\"].apply(str).apply(find_positive)\n",
    "tweets_fr_bert[\"negative\"] = tweets_fr_bert[\"sent_score\"].apply(str).apply(find_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_fr_bert[\"output\"] = tweets_fr_bert[\"output\"].str.strip(\"=\")\n",
    "tweets_fr_bert[\"neutral\"] = tweets_fr_bert[\"neutral\"].str.strip(\"NEU: \")\n",
    "tweets_fr_bert[\"positive\"] = tweets_fr_bert[\"positive\"].str.strip(\"POS: \")\n",
    "tweets_fr_bert[\"negative\"] = tweets_fr_bert[\"negative\"].str.strip(\"NEG: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_fr_bert[\"neutral\"] = tweets_fr_bert[\"neutral\"].astype('float64')\n",
    "tweets_fr_bert[\"positive\"] = tweets_fr_bert[\"positive\"].astype('float64')\n",
    "tweets_fr_bert[\"negative\"] = tweets_fr_bert[\"negative\"].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "tweets_fr_bert[\"output\"] = tweets_fr_bert[\"output\"].replace([\"NEU\", \"POS\", \"NEG\"], [\"neutral\", \"positive\", \"negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_fr_bert = tweets_fr_bert.drop(columns=[\"sent_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes together\n",
    "tweets_fr_bert_all = tweets_fr.merge(tweets_fr_bert, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file\n",
    "#tweets_fr_bert_all.to_csv('../data/tweets_fr_sa_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reopen file\n",
    "tweets_fr_sa_bert = pd.read_csv('../data/tweets_fr_sa_bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean of sentiments\n",
    "tweets_fr_bert_gp = tweets_fr_sa_bert.groupby(\"vaccine\")[\"neutral\", \"positive\", \"negative\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt the dataframe for plot\n",
    "tweets_fr_bert_melt = pd.melt(tweets_fr_bert_gp, id_vars=['vaccine'], value_vars=['negative','neutral', 'positive'], var_name='sentiment', value_name='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean values\n",
    "tweets_fr_bert_melt[\"vaccine\"] = tweets_fr_bert_melt[\"vaccine\"].replace([\"pfizer\", \"moderna\", \"astrazeneca\", \"johnson\"], [\"Pfizer\", \"Moderna\", \"AstraZeneca\", \"Johnson&Johnson\"])\n",
    "tweets_fr_bert_melt[\"sentiment\"] = tweets_fr_bert_melt[\"sentiment\"].replace([\"neutral\", \"positive\", \"negative\"], [\"Neutral\", \"Negative\", \"Positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# France\n",
    "fig = px.bar(tweets_fr_bert_melt, x='vaccine', y='mean', color='sentiment', barmode='stack', title='Sentiments on COVID-19 vaccination in France',\n",
    "             labels={\n",
    "                 \"mean\": \"\",\n",
    "                 \"vaccine\": \"\"\n",
    "                 \n",
    "             },\n",
    "            color_discrete_map={ # replaces default color mapping by value\n",
    "                \"Negative\": \"midnightblue\", \"Neutral\": \"steelblue\",\"Positive\": \"lightsteelblue\"\n",
    "            },\n",
    "            category_orders={\"vaccine\": [\"Pfizer\", \"Moderna\",\"AstraZeneca\", \"Johnson&Johnson\"],\n",
    "                            \"sentiment\": [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "                        },\n",
    "             template=\"simple_white\"\n",
    "            )\n",
    "fig.update_yaxes(showgrid=True, showline=False, tickwidth=0, tickcolor='white')\n",
    "fig.update_xaxes(showline=True, zeroline=True)\n",
    "fig.update_layout(legend_traceorder=\"reversed\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#py.plot(fig, filename = 'sentiments_vaccines_france', auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USA NORTHEAST:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define function to get sentiments\n",
    "def sentiment_analysis_us(df):\n",
    "    sen_ana=[]\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            texts = row[\"text\"]\n",
    "            tweet_id = row[\"id\"]\n",
    "            result = analyzer.predict(texts)\n",
    "            x = [tweet_id, result]\n",
    "            sen_ana.append(x)\n",
    "        except:\n",
    "            pass\n",
    "    result_df = pd.DataFrame(sen_ana, columns=[\"id\", \"sent_score\"])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ne_bert = sentiment_analysis_us(tweets_us_north)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ne_bert[\"output\"] = tweets_ne_bert[\"sent_score\"].apply(str).apply(find_output)\n",
    "tweets_ne_bert[\"neutral\"] = tweets_ne_bert[\"sent_score\"].apply(str).apply(find_neutral)\n",
    "tweets_ne_bert[\"positive\"] = tweets_ne_bert[\"sent_score\"].apply(str).apply(find_positive)\n",
    "tweets_ne_bert[\"negative\"] = tweets_ne_bert[\"sent_score\"].apply(str).apply(find_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ne_bert[\"output\"] = tweets_ne_bert[\"output\"].str.strip(\"=\")\n",
    "tweets_ne_bert[\"neutral\"] = tweets_ne_bert[\"neutral\"].str.strip(\"NEU: \")\n",
    "tweets_ne_bert[\"positive\"] = tweets_ne_bert[\"positive\"].str.strip(\"POS: \")\n",
    "tweets_ne_bert[\"negative\"] = tweets_ne_bert[\"negative\"].str.strip(\"NEG: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ne_bert[\"neutral\"] = tweets_ne_bert[\"neutral\"].astype('float64')\n",
    "tweets_ne_bert[\"positive\"] = tweets_ne_bert[\"positive\"].astype('float64')\n",
    "tweets_ne_bert[\"negative\"] = tweets_ne_bert[\"negative\"].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ne_bert[\"output\"] = tweets_ne_bert[\"output\"].replace([\"NEU\", \"POS\", \"NEG\"], [\"Neutral\", \"Positive\", \"Negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ne_bert_all = tweets_us_north.merge(tweets_ne_bert, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file\n",
    "#tweets_ne_bert_all.to_csv('../data/tweets_ne_sa_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ne_bert_all = pd.read_csv('../data/tweets_ne_sa_bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ne_bert_all_gp = tweets_ne_bert_all.groupby(\"vaccine\")[\"neutral\", \"positive\", \"negative\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ne_bert_melt = pd.melt(tweets_ne_bert_all_gp, id_vars=['vaccine'], value_vars=['negative','neutral', 'positive'], var_name='sentiment', value_name='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ne_bert_melt[\"vaccine\"] = tweets_ne_bert_melt[\"vaccine\"].replace([\"pfizer\", \"moderna\", \"astrazeneca\", \"johnson\"], [\"Pfizer\", \"Moderna\", \"AstraZeneca\", \"Johnson&Johnson\"])\n",
    "tweets_ne_bert_melt[\"sentiment\"] = tweets_ne_bert_melt[\"sentiment\"].replace([\"neutral\", \"positive\", \"negative\"], [\"Neutral\", \"Negative\", \"Positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USA NE\n",
    "fig = px.bar(tweets_ne_bert_melt, x='vaccine', y='mean', color='sentiment', barmode='stack', title='Sentiments on COVID-19 Vaccination in Northeast States',\n",
    "             labels={\n",
    "                 \"mean\": \"\",\n",
    "                 \"vaccine\": \"\"\n",
    "                 \n",
    "             },\n",
    "            color_discrete_map={ # replaces default color mapping by value\n",
    "                \"Negative\": \"darkgreen\", \"Neutral\": \"forestgreen\",\"Positive\": \"yellowgreen\"\n",
    "            },\n",
    "            category_orders={\"vaccine\": [\"Pfizer\", \"Moderna\",\"AstraZeneca\", \"Johnson&Johnson\"],\n",
    "                            \"sentiment\": [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "                        },\n",
    "             template=\"simple_white\"\n",
    "            )\n",
    "fig.update_yaxes(showgrid=True, showline=False, tickwidth=0, tickcolor='white')\n",
    "fig.update_xaxes(showline=True, zeroline=True)\n",
    "fig.update_layout(legend_traceorder=\"reversed\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#py.plot(fig, filename = 'sentiments_vaccines_usa_ne', auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USA SOUTHEAST:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_bert = sentiment_analysis_us(tweets_us_south)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_bert[\"output\"] = tweets_se_bert[\"sent_score\"].apply(str).apply(find_output)\n",
    "tweets_se_bert[\"neutral\"] = tweets_se_bert[\"sent_score\"].apply(str).apply(find_neutral)\n",
    "tweets_se_bert[\"positive\"] = tweets_se_bert[\"sent_score\"].apply(str).apply(find_positive)\n",
    "tweets_se_bert[\"negative\"] = tweets_se_bert[\"sent_score\"].apply(str).apply(find_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_bert[\"output\"] = tweets_se_bert[\"output\"].str.strip(\"=\")\n",
    "tweets_se_bert[\"neutral\"] = tweets_se_bert[\"neutral\"].str.strip(\"NEU: \")\n",
    "tweets_se_bert[\"positive\"] = tweets_se_bert[\"positive\"].str.strip(\"POS: \")\n",
    "tweets_se_bert[\"negative\"] = tweets_se_bert[\"negative\"].str.strip(\"NEG: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_bert[\"neutral\"] = tweets_se_bert[\"neutral\"].astype('float64')\n",
    "tweets_se_bert[\"positive\"] = tweets_se_bert[\"positive\"].astype('float64')\n",
    "tweets_se_bert[\"negative\"] = tweets_se_bert[\"negative\"].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_bert[\"output\"] = tweets_se_bert[\"output\"].replace([\"NEU\", \"POS\", \"NEG\"], [\"Neutral\", \"Positive\", \"Negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_bert_all = tweets_us_south.merge(tweets_ne_bert, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file\n",
    "#tweets_se_bert_all.to_csv('../data/tweets_se_sa_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_bert_all = pd.read_csv('../data/tweets_se_sa_bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_bert_all_gp = tweets_se_bert_all.groupby(\"vaccine\")[\"neutral\", \"positive\", \"negative\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_bert_melt = pd.melt(tweets_se_bert_all_gp, id_vars=['vaccine'], value_vars=['negative','neutral', 'positive'], var_name='sentiment', value_name='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_bert_melt[\"vaccine\"] = tweets_se_bert_melt[\"vaccine\"].replace([\"pfizer\", \"moderna\", \"astrazeneca\", \"johnson\"], [\"Pfizer\", \"Moderna\", \"AstraZeneca\", \"Johnson&Johnson\"])\n",
    "tweets_se_bert_melt[\"sentiment\"] = tweets_se_bert_melt[\"sentiment\"].replace([\"neutral\", \"positive\", \"negative\"], [\"Neutral\", \"Negative\", \"Positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USA SE\n",
    "fig = px.bar(tweets_se_bert_melt, x='vaccine', y='mean', color='sentiment', barmode='stack', title='Sentiments on COVID-19 Vaccination in Northeast States',\n",
    "             labels={\n",
    "                 \"mean\": \"\",\n",
    "                 \"vaccine\": \"\"\n",
    "                 \n",
    "             },\n",
    "            color_discrete_map={ # replaces default color mapping by value\n",
    "                \"Negative\": \"maroon\", \"Neutral\": \"indianred\",\"Positive\": \"navajowhite\"\n",
    "            },\n",
    "            category_orders={\"vaccine\": [\"Pfizer\", \"Moderna\",\"AstraZeneca\", \"Johnson&Johnson\"],\n",
    "                            \"sentiment\": [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "                        },\n",
    "             template=\"simple_white\"\n",
    "            )\n",
    "fig.update_yaxes(showgrid=True, showline=False, tickwidth=0, tickcolor='white')\n",
    "fig.update_xaxes(showline=True, zeroline=True)\n",
    "fig.update_layout(legend_traceorder=\"reversed\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#py.plot(fig, filename = 'sentiments_vaccines_usa_se', auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Perform sentiment analysis classification by using pretrained model â€œcardifnlp/twitter-roberta-base-emotion â€:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet emotion detection classifier\n",
    "task='emotion'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download label mapping\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model using pytorch\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FRANCE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get emotion for french tweets\n",
    "scores=[]\n",
    "for index, row in tweets_fr.iterrows():\n",
    "    #\n",
    "    try:\n",
    "        text = row[\"translated_text\"]\n",
    "        tweet_id = row[\"id\"]\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "        output = model(**encoded_input)\n",
    "        score = output[0][0].detach().numpy()\n",
    "        score = softmax(score)\n",
    "    except:\n",
    "        pass    \n",
    "    ranking = np.argsort(score)\n",
    "    ranking = ranking[::-1]\n",
    "    for i in range(score.shape[0]):\n",
    "        l = labels[ranking[i]]\n",
    "        s = score[ranking[i]]\n",
    "        x = [tweet_id,l,np.round(float(s), 4)]\n",
    "        scores.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the result to a dataframe\n",
    "tweets_fr_emotions = pd.DataFrame(scores, columns=[\"id\", \"emotion\", \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot the table\n",
    "tweets_fr_emotions_pv = pd.pivot_table(tweets_fr_emotions, values='score', columns='emotion', index=\"id\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get spider plots for emotion by vaccine:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the table with table \n",
    "tweets_fr_emotions_all = tweets_fr.merge(tweets_fr_emotions_pv, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#save file\n",
    "#tweets_fr_emotions_all.to_csv('../data/tweets_fr_emotions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_fr_emotions = pd.read_csv('../data/tweets_fr_emotions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean of each emotion\n",
    "tweets_fr_emotions_gp = tweets_fr_emotions.groupby(\"vaccine\")[\"anger\", \"joy\", \"optimism\", \"sadness\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_fr_emotions_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pfizer\n",
    "fig = go.Figure(data=go.Scatterpolar(\n",
    "  r=tweets_fr_emotions_gp.iloc[3,1:5],\n",
    "  theta=['anger', 'joy','optimism','sadness'],\n",
    "  fill='toself'))\n",
    "\n",
    "fig.update_layout(\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "        range=[0,0.5]\n",
    "    ),\n",
    "  ),\n",
    "  showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moderna\n",
    "fig = go.Figure(data=go.Scatterpolar(\n",
    "  r=tweets_fr_emotions_gp.iloc[2,1:5],\n",
    "  theta=['anger', 'joy','optimism','sadness'],\n",
    "  fill='toself'))\n",
    "\n",
    "fig.update_layout(\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "        range=[0,0.5]\n",
    "    ),\n",
    "  ),\n",
    "  showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#J&J\n",
    "fig = go.Figure(data=go.Scatterpolar(\n",
    "  r=tweets_fr_emotions_gp.iloc[1,1:5],\n",
    "  theta=['anger', 'joy','optimism','sadness'],\n",
    "  fill='toself'))\n",
    "\n",
    "fig.update_layout(\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "        range=[0,0.5]\n",
    "    ),\n",
    "  ),\n",
    "  showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Astra\n",
    "fig = go.Figure(data=go.Scatterpolar(\n",
    "  r= tweets_fr_emotions_gp.iloc[0,1:5],\n",
    "  theta=['anger', 'joy','optimism','sadness'],\n",
    "  fill='toself'))\n",
    "\n",
    "fig.update_layout(\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "        range=[0,0.5]\n",
    "    ),\n",
    "  ),\n",
    "  showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USA NORTHEAST:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get emotions in US NE\n",
    "def get_emotion(df):\n",
    "    scores=[]\n",
    "    for index, row in df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        tweet_id = row[\"id\"]\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "        output = model(**encoded_input)\n",
    "        score = output[0][0].detach().numpy()\n",
    "        score = softmax(score)\n",
    "        \n",
    "        ranking = np.argsort(score)\n",
    "        ranking = ranking[::-1]\n",
    "        for i in range(score.shape[0]):\n",
    "            l = labels[ranking[i]]\n",
    "            s = score[ranking[i]]\n",
    "            x = [tweet_id,l,np.round(float(s), 4)]\n",
    "            scores.append(x)\n",
    "    result_df = pd.DataFrame(scores, columns=[\"id\", \"emotion\", \"score\"])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function on US NE Tweets\n",
    "tweets_ne_emotion = get_emotion(tweets_us_north)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot the result\n",
    "tweets_ne_emotions_pv = pd.pivot_table(tweets_ne_emotion, values='score', columns='emotion', index=\"id\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge emotion results with US NE Tweets\n",
    "tweets_ne_emotions_all = tweets_us_north.merge(tweets_ne_emotions_pv, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean of each emotion per vaccine\n",
    "tweets_ne_emotions_gp = tweets_ne_emotions_all.groupby(\"vaccine\")[\"anger\", \"joy\", \"optimism\", \"sadness\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pfizer\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "                go.Scatterpolar(\n",
    "                                r=tweets_ne_emotions_gp.iloc[3,1:5],\n",
    "                                theta=['anger', 'joy','optimism','sadness'],\n",
    "                                fill='toself',\n",
    "                                \n",
    "                                fillcolor=\"yellowgreen\", opacity=0.6, line=dict(color=\"green\")\n",
    "                                )\n",
    "                )\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "            range=[0,0.5]\n",
    "        ),\n",
    "      ),\n",
    "    showlegend=False,\n",
    "    \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moderna\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "                go.Scatterpolar(\n",
    "                                r=tweets_ne_emotions_gp.iloc[2,1:5],\n",
    "                                theta=['anger', 'joy','optimism','sadness'],\n",
    "                                fill='toself',\n",
    "                                \n",
    "                                fillcolor=\"yellowgreen\", opacity=0.6, line=dict(color=\"green\")\n",
    "                                )\n",
    "                )\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "            range=[0,0.5]\n",
    "        ),\n",
    "      ),\n",
    "    showlegend=False,\n",
    "    \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Johnson&Johnson\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "                go.Scatterpolar(\n",
    "                                r=tweets_ne_emotions_gp.iloc[1,1:5],\n",
    "                                theta=['anger', 'joy','optimism','sadness'],\n",
    "                                fill='toself',\n",
    "                                \n",
    "                                fillcolor=\"yellowgreen\", opacity=0.6, line=dict(color=\"green\")\n",
    "                                )\n",
    "                )\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "            range=[0,0.5]\n",
    "        ),\n",
    "      ),\n",
    "    showlegend=False,\n",
    "    \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Astra\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "                go.Scatterpolar(\n",
    "                                r=tweets_ne_emotions_gp.iloc[0,1:5],\n",
    "                                theta=['anger', 'joy','optimism','sadness'],\n",
    "                                fill='toself',\n",
    "                                \n",
    "                                fillcolor=\"yellowgreen\", opacity=0.6, line=dict(color=\"green\")\n",
    "                                )\n",
    "                )\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "            range=[0,0.5]\n",
    "        ),\n",
    "      ),\n",
    "    showlegend=False,\n",
    "    \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USA SOUTHEAST:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function for US SE Tweets\n",
    "tweets_se_emotion = get_emotion(tweets_us_south)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_emotions_pv = pd.pivot_table(tweets_se_emotion, values='score', columns='emotion', index=\"id\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_emotions_all = tweets_us_south.merge(tweets_se_emotions_pv, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_se_emotions_gp = tweets_se_emotions_all.groupby(\"vaccine\")[\"anger\", \"joy\", \"optimism\", \"sadness\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pfizer\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "                go.Scatterpolar(\n",
    "                                r=tweets_se_emotions_gp.iloc[3,1:5],\n",
    "                                theta=['anger', 'joy','optimism','sadness'],\n",
    "                                fill='toself',\n",
    "                                \n",
    "                                fillcolor=\"indianred\", opacity=0.6, line=dict(color=\"maroon\")\n",
    "                                )\n",
    "                )\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "            range=[0,0.5]\n",
    "        ),\n",
    "      ),\n",
    "    showlegend=False,\n",
    "    \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moderna\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "                go.Scatterpolar(\n",
    "                                r=tweets_se_emotions_gp.iloc[2,1:5],\n",
    "                                theta=['anger', 'joy','optimism','sadness'],\n",
    "                                fill='toself',\n",
    "                                \n",
    "                                fillcolor=\"indianred\", opacity=0.6, line=dict(color=\"maroon\")\n",
    "                                )\n",
    "                )\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "            range=[0,0.5]\n",
    "        ),\n",
    "      ),\n",
    "    showlegend=False,\n",
    "    \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Johnson\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "                go.Scatterpolar(\n",
    "                                r=tweets_se_emotions_gp.iloc[1,1:5],\n",
    "                                theta=['anger', 'joy','optimism','sadness'],\n",
    "                                fill='toself',\n",
    "                                \n",
    "                                fillcolor=\"indianred\", opacity=0.6, line=dict(color=\"maroon\")\n",
    "                                )\n",
    "                )\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "            range=[0,0.5]\n",
    "        ),\n",
    "      ),\n",
    "    showlegend=False,\n",
    "    \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Astra\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "                go.Scatterpolar(\n",
    "                                r=tweets_se_emotions_gp.iloc[0,1:5],\n",
    "                                theta=['anger', 'joy','optimism','sadness'],\n",
    "                                fill='toself',\n",
    "                                \n",
    "                                fillcolor=\"indianred\", opacity=0.6, line=dict(color=\"maroon\")\n",
    "                                )\n",
    "                )\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "            range=[0,0.5]\n",
    "        ),\n",
    "      ),\n",
    "    showlegend=False,\n",
    "    \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Word frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize the stopwords\n",
    "custom_stopwords = STOPWORDS.union(set(['doses','dose','vaccination','vaccinated','vaccine', 'vaccines','coronavirus', 'covid','pfizer', 'astrazeneca', 'moderna', 'johnson', 'janssen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to get word frequency\n",
    "def word_frequency(df):\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1,1),\n",
    "        analyzer='word',\n",
    "        stop_words=custom_stopwords\n",
    "    )\n",
    "    \n",
    "    # Remove short words, pumctuation, numbers and special characters\n",
    "    sparse_matrix = word_vectorizer.fit_transform(\n",
    "        df[\"translated_text\"].apply(lambda x: \" \".join([x for x in simple_preprocess(x) if len(x)>3])\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    \n",
    "    # Create frequency matrix\n",
    "    frequencies = sparse_matrix.sum(axis=0)\n",
    "    \n",
    "    # Create DF from frequency matrix\n",
    "    result_df = pd.DataFrame(frequencies.reshape(-1,1), index=word_vectorizer.get_feature_names(), columns=['frequency'])\n",
    "    \n",
    "    # Return sorted DF\n",
    "    return result_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate Tweets if either positive or negative\n",
    "tweets_fr_pos = tweets_fr[tweets_fr.label == 'POSITIVE']\n",
    "tweets_fr_neg = tweets_fr[tweets_fr.label == 'NEGATIVE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the function to get word frequency\n",
    "freq_tweets_fr_pos = word_frequency(tweets_fr_pos)\n",
    "freq_tweets_fr_neg = word_frequency(tweets_fr_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the result\n",
    "freq_tweets_fr_pos = freq_tweets_fr_pos.reset_index()\n",
    "freq_tweets_fr_neg = freq_tweets_fr_neg.reset_index()\n",
    "\n",
    "freq_tweets_fr_pos = freq_tweets_fr_pos.rename(columns={'index':'words'})\n",
    "freq_tweets_fr_neg = freq_tweets_fr_neg.rename(columns={'index':'words'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 5 frequent words\n",
    "display(freq_tweets_fr_pos.sort_values(\"frequency\", ascending=False).head(5))\n",
    "display(freq_tweets_fr_neg.sort_values(\"frequency\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tuples for wordclouds\n",
    "tuples_fr_pos = [tuple(x) for x in freq_tweets_fr_pos.values]\n",
    "tuples_fr_neg = [tuple(x) for x in freq_tweets_fr_neg.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wordcloud positive french tweets\n",
    "plt.figure(figsize= (12, 8))\n",
    "wordcloud = WordCloud(width = 1500, height = 1000,\n",
    "                      random_state=1, background_color='lightgray', colormap='Set1',collocations=False).generate_from_frequencies(dict(tuples_fr_pos))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save image\n",
    "#wordcloud.to_file('../wordcloud/wordcloud_france_pos.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud negative french tweets\n",
    "plt.figure(figsize= (12, 8))\n",
    "wordcloud = WordCloud(width = 1500, height = 1000,\n",
    "                      random_state=1, background_color='black', colormap='Set2',collocations=False).generate_from_frequencies(dict(tuples_fr_neg))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save image\n",
    "#wordcloud.to_file('../wordcloud/wordcloud_france_neg.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as french Tweets function with few modifications\n",
    "def word_frequency_us(df):\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1,1),\n",
    "        analyzer='word',\n",
    "        stop_words=custom_stopwords\n",
    "    )\n",
    "    \n",
    "    # Remove short words, pumctuation, numbers and special characters\n",
    "    sparse_matrix = word_vectorizer.fit_transform(\n",
    "        df[\"text\"].apply(lambda x: \" \".join([x for x in simple_preprocess(x) if len(x)>3])\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    \n",
    "    # Create frequency matrix\n",
    "    frequencies = sparse_matrix.sum(axis=0)\n",
    "    \n",
    "    # Create DF from frequency matrix\n",
    "    result_df = pd.DataFrame(frequencies.reshape(-1,1), index=word_vectorizer.get_feature_names(), columns=['frequency'])\n",
    "    \n",
    "    # Return sorted DF\n",
    "    return result_df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**US NE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_north_pos = tweets_us_north[tweets_us_north.label == 'POSITIVE']\n",
    "tweets_us_north_neg = tweets_us_north[tweets_us_north.label == 'NEGATIVE']\n",
    "\n",
    "tweets_us_south_pos = tweets_us_south[tweets_us_south.label == 'POSITIVE']\n",
    "tweets_us_south_neg = tweets_us_south[tweets_us_south.label == 'NEGATIVE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tweets_us_north_pos = word_frequency_us(tweets_us_north_pos)\n",
    "freq_tweets_us_north_neg = word_frequency_us(tweets_us_north_neg)\n",
    "\n",
    "freq_tweets_us_south_pos = word_frequency_us(tweets_us_south_pos)\n",
    "freq_tweets_us_south_neg = word_frequency_us(tweets_us_south_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tweets_us_north_pos = freq_tweets_us_north_pos.reset_index()\n",
    "freq_tweets_us_north_neg = freq_tweets_us_north_neg.reset_index()\n",
    "\n",
    "freq_tweets_us_south_pos = freq_tweets_us_south_pos.reset_index()\n",
    "freq_tweets_us_south_neg = freq_tweets_us_south_neg.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tweets_us_north_pos = freq_tweets_us_north_pos.rename(columns={'index':'words'})\n",
    "freq_tweets_us_north_neg = freq_tweets_us_north_neg.rename(columns={'index':'words'})\n",
    "\n",
    "freq_tweets_us_south_pos = freq_tweets_us_south_pos.rename(columns={'index':'words'})\n",
    "freq_tweets_us_south_neg = freq_tweets_us_south_neg.rename(columns={'index':'words'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_us_n_pos = [tuple(x) for x in freq_tweets_us_north_pos.values]\n",
    "tuples_us_n_neg = [tuple(x) for x in freq_tweets_us_north_neg.values]\n",
    "\n",
    "tuples_us_s_pos = [tuple(x) for x in freq_tweets_us_south_pos.values]\n",
    "tuples_us_s_neg = [tuple(x) for x in freq_tweets_us_south_neg.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 frequent words\n",
    "display(freq_tweets_us_north_pos.sort_values(\"frequency\", ascending=False).head(5))\n",
    "display(freq_tweets_us_north_neg.sort_values(\"frequency\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud positive US NE tweets\n",
    "plt.figure(figsize= (12, 8))\n",
    "wordcloud = WordCloud(width = 1500, height = 1000,\n",
    "                      random_state=1, background_color='lightgray', colormap='Set1',collocations=False).generate_from_frequencies(dict(tuples_us_n_pos))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save image\n",
    "#wordcloud.to_file('../wordcloud/wordcloud_us_north_pos.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud negative US NE tweets\n",
    "plt.figure(figsize= (12, 8))\n",
    "wordcloud = WordCloud(width = 1500, height = 1000,\n",
    "                      random_state=1, background_color='black', colormap='Set2',collocations=False).generate_from_frequencies(dict(tuples_us_n_neg))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save image\n",
    "#wordcloud.to_file('../wordcloud/wordcloud_us_north_neg.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**US SE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 frequent words\n",
    "display(freq_tweets_us_south_pos.sort_values(\"frequency\", ascending=False).head(5))\n",
    "display(freq_tweets_us_south_neg.sort_values(\"frequency\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (12, 8))\n",
    "wordcloud = WordCloud(width = 1500, height = 1000,\n",
    "                      random_state=1, background_color='lightgray', colormap='Set1',collocations=False).generate_from_frequencies(dict(tuples_us_s_pos))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save image\n",
    "wordcloud.to_file('../wordcloud/wordcloud_us_south_pos.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (12, 8))\n",
    "wordcloud = WordCloud(width = 1500, height = 1000,\n",
    "                      random_state=1, background_color='black', colormap='Set2',collocations=False).generate_from_frequencies(dict(tuples_us_s_neg))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save image\n",
    "wordcloud.to_file('../wordcloud/wordcloud_us_south_neg.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Topic modeling - Latent Dirichlet Allocation (LDA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# customize stop words from NLTK \n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['vaccine', 'covid', 'coronavirus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run files\n",
    "tweets_fr_sa = pd.read_csv(\"../data/tweet_fr_sa.csv\")\n",
    "tweets_us_ne_sa = pd.read_csv(\"../data/tweets_us_north_sa.csv\")\n",
    "tweets_us_se_sa = pd.read_csv(\"../data/tweets_us_south_sa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FRANCE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dataframe\n",
    "tweets_fr_lda = tweets_fr_sa.drop(columns=[\"id\", \"date\", \"location\", \"follower_count\", \"retweets\", \"text\", \"sen_ana\", \"score\", \"vaccine\"])\n",
    "tweets_fr_lda = tweets_fr_lda.rename(columns ={'translated_text': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words smaller than 3 characters\n",
    "tweets_fr_lda[\"text\"] = tweets_fr_lda[\"text\"].apply(lambda x: \" \".join([x for x in simple_preprocess(x) if len(x)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = tweets_fr_lda.text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function using simple process from gensim\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models for lda\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en_core_web_sm' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.save_html(vis, '../html_links/lda_france.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USA NORTH:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_north_lda = tweets_us_ne_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_north_lda[\"text\"] = tweets_us_north_lda[\"text\"].apply(lambda x: \" \".join([x for x in simple_preprocess(x) if len(x)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = tweets_us_north_lda.text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en_core_web_sm' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.save_html(vis, '../html_links/lda_us_north.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USA SOUTH:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_south_lda = tweets_us_se_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_us_south_lda[\"text\"] = tweets_us_south_lda[\"text\"].apply(lambda x: \" \".join([x for x in simple_preprocess(x) if len(x)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = tweets_us_south_lda.text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en_core_web_sm' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.save_html(vis, '../html_links/lda_us_south.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
